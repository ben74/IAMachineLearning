# -*- coding: utf-8 -*-
"""ðŸ—²ðŸ—² IML 3 Seattle Power Plants ðŸ—²ðŸ—² 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eZWroiH3YneCaci2ccJQ9C4mfx-Bb7nM

# .----------------------------------------------------------------
"""



"""---
# ðŸ—² Seattle Buildings Power Consumption / GHG emissions Predictions ðŸ—²
---
<img src='https://i.snipboard.io/MGbyzf.jpg'/>

---

- This notebook begins with an Eda on building energy consumption ( electric, gaz, steam, miscellaneous ) and GHG emissions as an excuse to explore and compare different regression models ( excluding complex neural networks )
- Then we'll remove unique data which might lead to bad learning an remove closely correlated data in order to match our requirements : predict data when we don't have all the measurements for a building : aka : predict the consumption for a newly built building ( remove anything wich is energy related ), otherwise that would be too simple .. 
- Please note 1Kwh equals 3.412Kbtu, Logistic Regression works better as all data is normalized and some composite data might help us a lot in that process
---
- uses Alpow framework "alpow.py" mainly for save/load functions for resuming program state / training / exports

# .----------------------------------------------------------------
# Benchmarks > Determination coefficient rÂ² and Nb random keys
- Model : Best Score, Seed > Todo > Dict and Table {{}}
- Attention, car optimisation sur jeu de test, donc mesurer rÃ©sultats sur test !!
---
Enough with 60 keys ?
- PassiveAggressiveRegressorr2:0.88	rd:23
- LinearSVR :r2:0.87:rd:56
- XGBRegressor:r2:0.17:rd:1
- MLPRegressor	r2:0.82	rd:7
---
- Ridge	r2:0.29	rd:1
- ExtraTreesRegressor	r2:0.2	rd:2
- RandomForestRegressor	r2:0.17	rd:59
- AdaBoostRegressor	r2:-0.39	rd:43
- GradientBoostingRegressor	r2:0.18	rd:26
- HistGradientBoostingRegressor	r2:0.21	rd:1
---
Enough avec 30 clÃ©s ? Moins
- LinearSVR	r2:0.55	rd:27 < 0.87
- XGBRegressor	r2:0.17	rd:1
- PassiveAggressiveRegressor	r2:0.88	rd:23 (30)
- MLPRegressor	r2:0.82	rd:7
---
Enough 200 clÃ©s ? Aucun IntÃ©rÃªt dÃ©gradation passiveAgressive
- LinearSVR	r2:0.88	rd:61 =
- XGBRegressor	r2:0.17	rd:1 <
- PassiveAggressiveRegressor	r2:0.62	rd:187 <<
- MLPRegressor	r2:0.82	rd:7 <<<
---
Enough 10 clÃ©s ?
- LinearSVR	r2:0.55	rd:1 <
- XGBRegressor	r2:0.17	rd:1
- PassiveAggressiveRegressor	r2:0.89	rd:4
- MLPRegressor	r2:0.83	rd:2

---
#I) Library
- Including some helpfull functions
---
"""

import os;
os.system('rm -f gv.py alpow.py;wget https://alpow.fr/alpow.py;wget https://alpow.fr/gv.py');
if False:#freeze version at first run please !
  os.system('pip freeze > versions.txt')
  !sed -e ':a' -e 'N' -e '$!ba' -e 's/\n/ ; /g' versions.txt
requiredModules='Pillow==7.0.0 absl-py==0.10.0 alabaster==0.7.12 albumentations==0.1.12 altair==4.1.0 argon2-cffi==20.1.0 asgiref==3.3.0 astor==0.8.1 astropy==4.1 astunparse==1.6.3 async-generator==1.10 atari-py==0.2.6 atomicwrites==1.4.0 attrs==20.2.0 audioread==2.1.9 autograd==1.3 Babel==2.8.0 backcall==0.2.0 bcrypt==3.2.0 beautifulsoup4==4.6.3 bleach==3.2.1 blis==0.4.1 bokeh==2.1.1 Bottleneck==1.3.2 branca==0.4.1 bs4==0.0.1 CacheControl==0.12.6 cachetools==4.1.1 catalogue==1.0.0 certifi==2020.6.20 cffi==1.14.3 chainer==7.4.0 chardet==3.0.4 click==7.1.2 cloudpickle==1.3.0 cmake==3.12.0 cmdstanpy==0.9.5 colorlover==0.3.0 community==1.0.0b1 contextlib2==0.5.5 convertdate==2.2.2 coverage==3.7.1 coveralls==0.5 crcmod==1.7 cryptography==3.2.1 cufflinks==0.17.3 cvxopt==1.2.5 cvxpy==1.0.31 cycler==0.10.0 cymem==2.0.4 Cython==0.29.21 daft==0.0.4 dask==2.12.0 dataclasses==0.7 datascience==0.10.6 debugpy==1.0.0 decorator==4.4.2 defusedxml==0.6.0 descartes==1.1.0 dill==0.3.3 distributed==1.25.3 Django==3.1.3 dlib==19.18.0 dm-tree==0.1.5 docopt==0.6.2 docutils==0.16 dopamine-rl==1.0.5 earthengine-api==0.1.238 easydict==1.9 ecos==2.0.7.post1 editdistance==0.5.3 en-core-web-sm==2.2.5 entrypoints==0.3 ephem==3.7.7.1 et-xmlfile==1.0.1 fa2==0.3.5 fancyimpute==0.4.3 fastai==1.0.61 fastdtw==0.3.4 fastprogress==1.0.0 fastrlock==0.5 fbprophet==0.7.1 feather-format==0.4.1 filelock==3.0.12 firebase-admin==4.4.0 fix-yahoo-finance==0.0.22 Flask==1.1.2 folium==0.8.3 future==0.16.0 gast==0.3.3 GDAL==2.2.2 gdown==3.6.4 gensim==3.6.0 geographiclib==1.50 geopy==1.17.0 gin-config==0.3.0 glob2==0.7 google==2.0.3 google-api-core==1.16.0 google-api-python-client==1.7.12 google-auth==1.17.2 google-auth-httplib2==0.0.4 google-auth-oauthlib==0.4.2 google-cloud-bigquery==1.21.0 google-cloud-bigquery-storage==1.1.0 google-cloud-core==1.0.3 google-cloud-datastore==1.8.0 google-cloud-firestore==1.7.0 google-cloud-language==1.2.0 google-cloud-storage==1.18.1 google-cloud-translate==1.5.0 google-colab==1.0.0 google-pasta==0.2.0 google-resumable-media==0.4.1 googleapis-common-protos==1.52.0 googledrivedownloader==0.4 graphviz==0.10.1 grpcio==1.33.2 gspread==3.0.1 gspread-dataframe==3.0.8 gym==0.17.3 h5py==2.10.0 HeapDict==1.0.1 holidays==0.10.3 holoviews==1.13.5 html5lib==1.0.1 httpimport==0.5.18 httplib2==0.17.4 httplib2shim==0.0.3 humanize==0.5.1 hyperopt==0.1.2 ideep4py==2.0.0.post3 idna==2.10 image==1.5.33 imageio==2.4.1 imagesize==1.2.0 imbalanced-learn==0.4.3 imblearn==0.0 imgaug==0.2.9 importlib-metadata==2.0.0 importlib-resources==3.3.0 imutils==0.5.3 inflect==2.1.0 iniconfig==1.1.1 intel-openmp==2020.0.133 intervaltree==2.1.0 ipykernel==4.10.1 ipython==5.5.0 ipython-genutils==0.2.0 ipython-sql==0.3.9 ipywidgets==7.5.1 itsdangerous==1.1.0 jax==0.2.4 jaxlib==0.1.56+cuda101 jdcal==1.4.1 jedi==0.17.2 jieba==0.42.1 Jinja2==2.11.2 joblib==0.17.0 jpeg4py==0.1.4 jsonschema==2.6.0 jupyter==1.0.0 jupyter-client==5.3.5 jupyter-console==5.2.0 jupyter-core==4.6.3 jupyterlab-pygments==0.1.2 kaggle==1.5.9 kapre==0.1.3.1 Keras==2.4.3 Keras-Preprocessing==1.1.2 keras-vis==0.4.1 kiwisolver==1.3.1 knnimpute==0.1.0 korean-lunar-calendar==0.2.1 librosa==0.6.3 lightgbm==2.2.3 llvmlite==0.31.0 lmdb==0.99 lucid==0.3.8 LunarCalendar==0.0.9 lxml==4.2.6 Markdown==3.3.3 MarkupSafe==1.1.1 matplotlib==3.2.2 matplotlib-venn==0.11.6 missingno==0.4.2 mistune==0.8.4 mizani==0.6.0 mkl==2019.0 mlxtend==0.14.0 more-itertools==8.6.0 moviepy==0.2.3.5 mpmath==1.1.0 msgpack==1.0.0 multiprocess==0.70.10 multitasking==0.0.9 murmurhash==1.0.3 music21==5.5.0 natsort==5.5.0 nbclient==0.5.1 nbconvert==5.6.1 nbformat==5.0.8 nest-asyncio==1.4.2 networkx==2.5 nibabel==3.0.2 nltk==3.2.5 notebook==5.3.1 np-utils==0.5.12.1 numba==0.48.0 numexpr==2.7.1 numpy==1.18.5 nvidia-ml-py3==7.352.0 oauth2client==4.1.3 oauthlib==3.1.0 okgrade==0.4.3 opencv-contrib-python==4.1.2.30 opencv-python==4.1.2.30 openpyxl==2.5.9 opt-einsum==3.3.0 osqp==0.6.1 packaging==20.4 palettable==3.3.0 pandas==1.1.4 pandas-datareader==0.9.0 pandas-gbq==0.13.3 pandas-profiling==1.4.1 pandocfilters==1.4.3 panel==0.9.7 param==1.10.0 paramiko==2.7.2 parso==0.7.1 pathlib==1.0.1 patsy==0.5.1 pexpect==4.8.0 pickleshare==0.7.5 Pillow==7.0.0 pip-tools==4.5.1 plac==1.1.3 plotly==4.4.1 plotnine==0.6.0 pluggy==0.7.1 portpicker==1.3.1 prefetch-generator==1.0.1 preshed==3.0.2 prettytable==1.0.1 progressbar2==3.38.0 prometheus-client==0.8.0 promise==2.3 prompt-toolkit==1.0.18 protobuf==3.12.4 psutil==5.4.8 psycopg2==2.7.6.1 ptyprocess==0.6.0 py==1.9.0 pyarrow==0.14.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycocotools==2.0.2 pycparser==2.20 pyct==0.4.8 pydata-google-auth==1.1.0 pydot==1.3.0 pydot-ng==2.0.0 pydotplus==2.0.2 PyDrive==1.3.1 pyemd==0.5.1 pyglet==1.5.0 Pygments==2.6.1 pygobject==3.26.1 pymc3==3.7 PyMeeus==0.3.7 pymongo==3.11.0 pymystem3==0.2.0 PyNaCl==1.4.0 PyOpenGL==3.1.5 pyparsing==2.4.7 pyrsistent==0.17.3 pysftp==0.2.9 pysndfile==1.3.8 PySocks==1.7.1 pystan==2.19.1.1 pytest==3.6.4 python-apt==1.6.5+ubuntu0.3 python-chess==0.23.11 python-dateutil==2.8.1 python-louvain==0.14 python-slugify==4.0.1 python-utils==2.4.0 pytz==2018.9 pyviz-comms==0.7.6 PyWavelets==1.1.1 PyYAML==3.13 pyzmq==19.0.2 qtconsole==4.7.7 QtPy==1.9.0 regex==2019.12.20 requests==2.23.0 requests-oauthlib==1.3.0 resampy==0.2.2 retrying==1.3.3 rpy2==3.2.7 rsa==4.6 scikit-image==0.16.2 scikit-learn==0.22.2.post1 scipy==1.4.1 screen-resolution-extra==0.0.0 scs==2.1.2 seaborn==0.11.0 Send2Trash==1.5.0 setuptools-git==1.2 Shapely==1.7.1 simplegeneric==0.8.1 six==1.15.0 sklearn==0.0 sklearn-pandas==1.8.0 slugify==0.0.1 smart-open==3.0.0 snowballstemmer==2.0.0 sortedcontainers==2.2.2 spacy==2.2.4 Sphinx==1.8.5 sphinxcontrib-serializinghtml==1.1.4 sphinxcontrib-websupport==1.2.4 SQLAlchemy==1.3.20 sqlparse==0.4.1 srsly==1.0.2 statsmodels==0.10.2 sympy==1.1.1 tables==3.4.4 tabulate==0.8.7 tblib==1.7.0 tensorboard==2.3.0 tensorboard-plugin-wit==1.7.0 tensorboardcolab==0.0.22 tensorflow==2.3.0 tensorflow-addons==0.8.3 tensorflow-datasets==4.0.1 tensorflow-estimator==2.3.0 tensorflow-gcs-config==2.3.0 tensorflow-hub==0.10.0 tensorflow-metadata==0.24.0 tensorflow-privacy==0.2.2 tensorflow-probability==0.11.0 termcolor==1.1.0 terminado==0.9.1 testpath==0.4.4 text-unidecode==1.3 textblob==0.15.3 textgenrnn==1.4.1 Theano==1.0.5 thinc==7.4.0 tifffile==2020.9.3 toml==0.10.2 toolz==0.11.1 torch==1.7.0+cu101 torchsummary==1.5.1 torchtext==0.3.1 torchvision==0.8.1+cu101 tornado==5.1.1 tqdm==4.41.1 traitlets==4.3.3 tweepy==3.6.0 typeguard==2.7.1 typing-extensions==3.7.4.3 tzlocal==1.5.1 umap-learn==0.4.6 uritemplate==3.0.1 urllib3==1.24.3 vega-datasets==0.8.0 wasabi==0.8.0 wcwidth==0.2.5 webencodings==0.5.1 webptools==0.0.3 Werkzeug==1.0.1 wget==3.2 widgetsnbextension==3.5.1 wordcloud==1.5.0 wrapt==1.12.1 xarray==0.15.1 xgboost==0.90 xkit==0.0.0 xlrd==1.1.0 xlwt==1.3.0 yellowbrick==0.9.1 zict==2.0.0 zipp==3.4.0'
import alpow;import alpow;from alpow import *

"""---
## Other functions
- Variables and stuff
"""

#}IO{
noPG=0;#ignores parameters grid .. if it takes too much time
dpi=100  
my_cv=5;#20 nbFolds
nbRdKeys=2
scoring='r2';#neg_mean_squared_error
votingOnReel=1
lastVotingKey=''
nbBestModel4Voting=3
SG('webRepo','http://1.x24.fr/a/jupyter/')

npRandomKeys=randomKeys=[1,2,3,4,12,24,42,68,124,256]
nbRdKeysPerModel={'LinearSVR':56,'XGBRegressor':10,'PassiveAggressiveRegressor':20,'MLPRegressor':10}
nbRdKeysPerModel={'LinearSVR':3,'XGBRegressor':3,'PassiveAggressiveRegressor':3,'MLPRegressor':3}
energies='ENERGYSTARScore,Electricity(kBtu),SiteEUI(kBtu/sf),SiteEUIWN(kBtu/sf),SiteEnergyUse(kBtu),SiteEnergyUseWN(kBtu),SourceEUI(kBtu/sf),SourceEUIWN(kBtu/sf),GHGEmissions(MetricTonsCO2e),GHGEmissionsIntensity(kgCO2e/ft2),SteamUse(kBtu),NaturalGas(kBtu)'.split(',')
toPredictBase=toPredict='Electricity(kBtu),ENERGYSTARScore,SiteEUI(kBtu/sf),SiteEUIWN(kBtu/sf),SourceEUI(kBtu/sf),SourceEUIWN(kBtu/sf),GHGEmissions(MetricTonsCO2e),GHGEmissionsIntensity(kgCO2e/ft2),SteamUse(kBtu),NaturalGas(kBtu),SiteEnergyUse(kBtu),SiteEnergyUseWN(kBtu)'.split(',')
#}{


jumpto=0;resumed=0;
allVars={'jumpto':1}
votedm={};k1=0;bestNP=[];previousR2=0;res={};pred={};acy={};nfm={};fi={};ev={};bestparams={};models={};ignoreParameters=0;bss={}
repeatedGridSearchCVSameResults={};bestRandomStateValues={};bestRandom={}

#on exclut ainsi les indices non reportÃ© au SQFT : #
pd.set_option('display.max_rows',900)
pd.set_option('display.max_columns',40)
pd.set_option('display.width',1200)#or evaluate js document frame innerWidth .
plt.style.use('fivethirtyeight')
plt.rcParams["figure.figsize"] = (24,12)
plt.rcParams['figure.facecolor'] = 'white'
#pandas as pd,numpy as np,matplotlib.pyplot as plt,
import math,gc,ast,json,hashlib,seaborn as sns,keras,sklearn.metrics,sklearn.model_selection,sklearn.svm,xgboost,numpy,pandas,sklearn.linear_model,sklearn.neural_network,sklearn.dummy,sklearn.metrics,sklearn.model_selection,sklearn.preprocessing,matplotlib,sklearn.ensemble

from mpl_toolkits import mplot3d
from keras.models import Sequential
from keras.optimizers import Adam,Adadelta,Nadam,RMSprop,Adamax,SGD
from keras.layers import Dense, Activation, Dropout
from pandas.plotting import scatter_matrix
from keras.optimizers import SGD  

from sklearn import ensemble, tree, linear_model,metrics,utils,preprocessing,metrics,feature_selection,model_selection
from sklearn.preprocessing import LabelEncoder,MinMaxScaler #Imputer,
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
#!pip install 
#import sklearn.cross_decomposition.PLSRegression

import sklearn.gaussian_process,sklearn.kernel_ridge
from sklearn.experimental import enable_hist_gradient_boosting
#from sklearn.linear_model import Ridge, BayesianRidge, LinearRegression, Lasso, ElasticNet,MultiTaskLasso,MultiTaskElasticNet,Lars,LassoLars,OrthogonalMatchingPursuit,ARDRegression,TheilSenRegressor,HuberRegressor,SGDRegressor

from sklearn.model_selection import train_test_split,StratifiedShuffleSplit,cross_val_score,GridSearchCV, StratifiedKFold
from sklearn.metrics import mean_squared_error,mean_squared_log_error,accuracy_score,average_precision_score,f1_score,balanced_accuracy_score,r2_score

from sklearn.ensemble import RandomForestClassifier,VotingClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron,SGDClassifier
from sklearn.tree import DecisionTreeClassifier


from keras.utils import to_categorical
from sklearn.utils import shuffle
from time import time

# Set default for pandas displays
#x1,y1,x2,y2,y_mean,mean=getSplitted(energy,dfNoEnergies)
def getSplitted(energy,_df):    
  global splittedData
  Hashv=str(hash(_df.values.tobytes()))
  k='labEnc-'+energy+'-'+Hashv
  if(k in splittedData):
    print(energy+' dataframe split exists')
    x1, y1, x2, y2, mean = splittedData[k]
  else:
    _df2=labelEncoded.copy()   
    if energy not in labEncCols: 
      _df2.insert(2,energy,dfNewKeys[energy].values,True) 
    results=_df2[energy].fillna(0).astype('float')
    mean=results.mean()      
    x1, y1, x2, y2 = ShuffleOrNot(_df,results)#labelEncoded.drop([energy],axis=1)
    splittedData[k]=[x1, y1, x2, y2,mean];

  y_mean = [mean]*len(y2)
  return x1,y1,x2,y2,y_mean,mean

def save(_globals,exclusions=[],fn='allVars',include=False):
    exclusions+='models,allVars,sftp'.split(',')        
    _vars={};_gk=list(_globals.keys())
    if(include):
        _gk=include
    for i in _gk:
        if '_' in i:
            continue;
        if i in _globals.keys():
            if type(_globals[i]) in [str,dict,list,int,pd.DataFrame,pd.Series]:#yyuuuuuu !
                _vars[i]=_globals[i];  
    size={};
    #x=%who_ls str dict list int DataFrame Series
    a=time()
    for i in exclusions:
        if i in _vars.keys():
            del _vars[i]
        
    for i in _vars:#compact
        size[i]=sys.getsizeof(_vars[i])
    print(arsort(size))    
    #print(','.join(x)+':: saved')
    os.system('rm '+fn+'.pickle '+fn+'.pickle.zip')
    FPCP(fn,_vars); 
    print('saved in:'+str(time()-a))
    o=subprocess.check_output('zip '+fn+'.pickle.zip '+fn+'.pickle', shell=True);print(o)
    now=datetime.datetime.now()
    fn2=fn+'.'+now.strftime("%Y%m%d-%H%M")+'.pickle.zip'
    os.system('cp '+fn+'.pickle.zip '+fn2)  
    ftpput([fn+'.pickle.zip',fn2])  

#x1, y1, x2, y2 = sklearn.model_selection.train_test_split(dfNoEnergies,results,train_size=0.8, test_size=0.2, random_state=100 )
shuffleData=0
def ShuffleOrNot(_df,results,train_size=0.8,test_size=0.2,random_state=100):
  if shuffleData:
#TypeError: train_test_split train_size Singleton array array cannot be considered a valid collection    
    return sklearn.model_selection.train_test_split(_df,results,train_size=train_size,test_size=test_size,random_state=random_state)
  print('non shuffled')
  return sklearn.model_selection.train_test_split(_df,results,train_size=train_size,test_size=test_size,shuffle=False)#, stratify = None  
  return non_shuffling_train_test_split(_df,results)  

def loadData(f,sep=','):
  return pd.read_csv(f,sep=',')

def pltinit(df,i,j):
    fig,ax=plt.subplots()  
    fig.patch.set_facecolor('white')
    x=df[i];y=df[j];
    plt.title(i+' vs '+j);plt.xlabel(i);plt.ylabel(j);
    return [x,y,filename(i+'.'+j),fig,ax];
    
def plot(df,i,j):
    x,y,fn,fig,ax=pltinit(df,i,j)
    #bestCorrelationsKeys.keys():
    plt.plot(x,y)
    plt.savefig('plot'+fn+'.png', bbox_inches = 'tight',dpi=dpi)
    show()

def scatter1(df,i,j,ts=0):
    x,y,fn,fig,ax=pltinit(df,i,j)  
    if ts:
      plt.gca().set_yticklabels(backgroundcolor='white',labels=y,rotation=(0),fontsize=ts,linespacing=ts)     
      #ax.tick_params(axis='y',which='major',pad=ts)
      #ax.set_yticklabels(labels=y,rotation = (45), fontsize = 10, va='bottom', ha='left')     
    plt.scatter(x,y);
    plt.savefig('scatter'+fn+'.png', bbox_inches = 'tight',dpi=dpi);ftpput('scatter'+fn+'.png')
    show();

#type(sup0)==pandas.core.series.Series

def md5(x):  
  return hashlib.md5(bencode.bencode(x)).hexdigest()        


def NewModel1(x_size, y_size):
    t_model = Sequential()
    t_model.add(Dense(100, activation="tanh", input_shape=(x_size,)))
    t_model.add(Dense(50, activation="relu"))
    t_model.add(Dense(y_size, activation='linear'))
    return(t_model)

def NewModel2(x_size, y_size):
    t_model = Sequential()
    t_model.add(Dense(100, activation="tanh", input_shape=(x_size,)))
    t_model.add(Dropout(0.1))
    t_model.add(Dense(50, activation="relu"))
    t_model.add(Dense(20, activation="relu"))
    t_model.add(Dense(y_size, activation='linear'))
    return(t_model)    

def NewModel3(x_size, y_size):
    t_model = Sequential()
    t_model.add(Dense(80, activation="tanh", kernel_initializer='normal', input_shape=(x_size,)))
    t_model.add(Dropout(0.2))
    t_model.add(Dense(120, activation="relu", kernel_initializer='normal', 
        kernel_regularizer=regularizers.l1(0.01), bias_regularizer=regularizers.l1(0.01)))
    t_model.add(Dropout(0.1))
    t_model.add(Dense(20, activation="relu", kernel_initializer='normal', 
        kernel_regularizer=regularizers.l1_l2(0.01), bias_regularizer=regularizers.l1_l2(0.01)))
    t_model.add(Dropout(0.1))
    t_model.add(Dense(10, activation="relu", kernel_initializer='normal'))
    t_model.add(Dropout(0.0))
    t_model.add(Dense(y_size, activation='linear'))
    return(t_model)    

def nn4(x_size, y_size):
  model = Sequential([
  Dense(32, activation='relu', input_shape=(x_size,)),
  Dense(32, activation='relu'),
  Dense(y_size, activation='sigmoid')
  ])
  #model.compile(optimizer='sgd',
  return model  

def histogram(x,name,score):  
  #k=x.history.keys();print(k)#dict_keys(['val_loss', 'val_mean_absolute_error', 'loss', 'mean_absolute_error'])#"mean_squared_error" 
  #mean_squared_logarithmic_error
  first=x.history['mean_squared_error'][:1] 
  last=x.history['mean_squared_error'][-1:]#TypeError: list indices must be integers or slices, not tuple
  #print(first[0]);print(last[0])
  progress=first[0]-last[0];
  print('Progress:'+str(progress))
  #plusieurs champs k
  for i in x.history.keys():
    plt.plot(x.history[i],label=i);
  plt.title(str(score)+'/'+name)
  x=filename(str(name))+'.histogram.png';
  plt.savefig(x,bbox_inches='tight');ftpput(x);plt.close()
  return progress
  show();

#Fit and Predict at the same time !!!
def fit(x,fn=0,noload=0):
  global x1,x2,y1,y2,ep,bs,k,pred,acy,k,toGuess;
  res=0;history=0
  if(fn):#changmenet de shape de donnÃ©es en input ..
    if(noload==0 & os.path.isfile(fn)==True):
      getFile(fn)
      x=keras.models.load_model(fn) 
      print('load model:'+fn)
      res=1;

    if(res==0):
      print('generate model:'+fn)
      history=x.fit(x1,x2,validation_data=(y1,y2),epochs=ep,batch_size=bs,shuffle=True,verbose=0)
      print('_'*160)
      #history=x.fit(standardize(x1),standardize(x2),validation_data=(standardize(y1),standardize(y2)),epochs=ep,batch_size=bs,shuffle=True,verbose=2)
      x.save(fn);ftpput(fn);#zipped ?    

  nfm[k]=x;
  pred[toGuess][k]=x.predict(y1);#mean_squared_log_error
#!!!:ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values.  
  acy[toGuess][k]=round(mean_squared_error(y2,pred[toGuess][k]) ** (1/2));
  say(acy[toGuess][k]);print(k+' => '+str(acy[toGuess][k]));
  if(history):
    histogram(history,k,acy[toGuess][k]);
  print('_'*120)

def r2(a,b):
  return stats.pearsonr(a,b)[0] ** 2

def standardize(df):
  mean = np.mean(df, axis=0)
  std = np.std(df, axis=0)#+0.000001
  return (df - mean) / std

def snsscat(df,a='',b='',ts=0,minx=0,maxx=0,miny=0,maxy=0,opacity=0.02,color='blue',size=2,axis=0,xscale=0,yscale=0,fn=0,kind='scatter'):
  if(fn==0):
    fn=filename(a+'.'+b+'.png'); 
  x=df[a];y=df[b];#fig,ax=plt.subplots(1);#ax=ax.flatten()
  slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)
  corr=stats.pearsonr(x, y)[0] ** 2
  #x,y,fn2,fig,ax=pltinit(df,a,b,axis)
  #ax.scatter(x,y,alpha=opacity,c=color,s=size);
  if(maxx==0):
    maxx=x.max()  
  if(maxy==0): 
    maxy=y.max()
#https://stackoverflow.com/questions/36191906/rescale-axis-on-seaborn-jointgrid-kde-marginal-plots    'ax':ax, 'height':height,
#https://seaborn.pydata.org/generated/seaborn.JointGrid.html
  args={'dropna':True,'kind':kind,'x':x,'y':y, 'data':df, 'joint_kws': {'alpha':opacity,'color':color},'xlim':[minx,maxx],'ylim':[miny,maxy],'color':'red'} #
  ##regplot() got an unexpected keyword argument 'stat_func'
  #args['stat_func']=r2
  args['marginal_kws']=dict(bins=15, rug=True)
  #args['annot_kws']=dict(stat="r");      #joinplot annot_kws
#N'apparait pas ..

  if(kind in ['reg']):
    args['joint_kws']={'color':color}#reg no opacity
    args['line_kws']={'label':"y={0:.1f}x+{1:.1f}".format(slope,intercept)};#not hex,nor kde  
    args['scatter_kws']={'alpha':opacity}; #,nor kde
  if(kind in ['kde']):    
    args['joint_kws']={'color':color}
    args['marginal_kws']={}#no bins property
    pass;

  #print(args);#TypeError: regplot() got an unexpected keyword argument 'alpha',dir(ax)
  g=sns.jointplot(**args) #unpack 
  #g=sns.JointGrid(**args)  
  """
  g = sns.JointGrid(x="total_bill", y="tip", data=tips)
  g = g.plot_joint(plt.scatter,color="g", s=40, edgecolor="white")
  g = g.plot_marginals(sns.distplot, kde=False, color="g")
  g = g.annotate(stats.pearsonr)

  g = g.plot_joint(sns.kdeplot, cmap="Blues_d")
  g = g.plot_marginals(sns.kdeplot, shade=True)
  """
  if(xscale):
    g.set_xscale(xscale)
  if(yscale):
    g.set_yscale(yscale)
  #ax.set_xlabel(a);ax.set_ylabel(b);#dÃ©jÃ  attribuÃ©s
#AttributeError: 'JointGrid' object has no attribute 'set_xlim'     
  if((kind in ['reg','kde']) & False):      
    regline=g.ax_joint.get_lines()[0]
    regline.set_color('red')
    regline.set_zorder('5')
  #fig.text(1, 1, "An annotation", horizontalalignment='left', size='medium', color='black', weight='semibold')
  #dir(g)
  #rsquare = lambda a, b: 
  #rsquare = lambda a, b: 2;g.annotate(rsquare)
  #ax = g.axes[0,0]
  plt.title(str(int(slope))+'-'+str(int(intercept)))
  g.savefig(fn, bbox_inches='tight');
  webp(fn)
  #ftpput(fn)     
  show()
  #plt.show()
  return 1;  

#snsscat(_df,'Real','Guess',opacity=0.2,color=[1,0,0.3,0.01],kind='reg',fn=filename('reg-'+toGuess+'-'+k)+'.png')#Reg  

def saveEvaluation():
  print('\n\n\n');
  message('evaluations saved')
  save(globals(),[],'modelsEvalutation-'+'-'+filename(','.join(models.keys())),'energy,dfn,r2s,pred,resultsArray,y_means,bestParameters,ev,acy'.split(','))

SG('webpBroken',1)
def scatter(df,a='',b='',ts=0,minx=0,maxx=0,miny=0,maxy=0,opacity=0.02,color='blue',size=2,axis=0,xscale=0,yscale=0,reg=0,fn=0):
  if(fn==0):
    fn=filename(a+'.'+b+'.png'); 
  #recombinÃ© avec son index  
  if(type(df)==pd.core.series.Series):#{
    p('serie')
    if(len(df)==0):
      print('empty.. skipping')
      return 0;   
    #x=df.index
    #print('serie')
    if(maxy==0):#maxx or de propos
      maxy=df.max()  
    x=range(df.shape[0])  

    if(axis):
      axis.scatter(x,df,s=size,alpha=opacity)
      #axis.set_title(fn)
      #  ax.set_xlim(minx,maxx);ax.set_ylim(miny,maxy)
      axis.set_xlabel(a)
      axis.set_ylabel(b)
      axis.set_facecolor('white')
      if(yscale):
        axis.set_yscale(yscale)
      if(xscale):
        axis.set_xscale(xscale)
      return 1;
    
    plt.figure().set_facecolor('white')
    plt.scatter(x,df,s=size,alpha=opacity)
    #plt.title(fn)
    if maxy:
      plt.ylim(miny,maxy)      
    plt.xlabel(a)
    plt.ylabel(b)
    if(yscale):
      plt.yscale(yscale)
    if(xscale):
      plt.xscale(xscale)
    plt.savefig(fn, bbox_inches='tight')
    #ftpput(fn) 
    webp(fn)#show()
    return fn;
#}end pandas serie    
### Sinon Dataframe ordinaire
  elif(axis==0):
    x=df[a];y=df[b]
    args={'x':x,'y':y, 'data':df, 'joint_kws': {'alpha':opacity,'color':color}}
    if(reg):
      args['kind']='reg';      
      args['joint_kws']={'color':color};     
      args['scatter_kws']={'alpha':opacity};     
      
    #print(args);#TypeError: regplot() got an unexpected keyword argument 'alpha'
    sns.jointplot(**args).savefig(fn, bbox_inches='tight');
    webp(fn)#show()
    return fn;
### Rendu sur un axe ..
  x,y,fn2,fig,ax=pltinit(df,a,b,axis)

  if(maxx==0):
    maxx=x.max()  
  if(maxy!=0): 
    maxy=y.max()

  if(axis!=0):#specifiÃ© non gÃ©nÃ©rÃ©
    axis.set_xlabel(a);axis.set_ylabel(b);
    axis.scatter(x,y,alpha=opacity,c=color,s=size);
    ax=axis

  ax.set_facecolor('white')

  if(xscale):
    ax.set_xscale(xscale)
  if(yscale):
    ax.set_yscale(yscale)
  return 1
  
  if(ts & False):
    plt.gca().set_yticklabels(backgroundcolor='white',labels=y,rotation=(0),fontsize=ts,linespacing=ts)     
    #ax.tick_params(axis='y',which='major',pad=ts)
    #ax.set_yticklabels(labels=y,rotation = (45), fontsize = 10, va='bottom', ha='left')      
  if(axis==0):    
    if False:    
      plt.scatter(x,y,alpha=opacity,c=color,s=size);    
      #show();
      plt.savefig(fn, bbox_inches='tight')    
      webp(fn)  

#scatter(train,'longitude','latitude',fn='eliottBayLatLonPrint.png')#  
def load2(fn='allVars', onlyIfNotSet=1):
    fns = fn.split(',')
    for fn in fns:
        fn = fn.strip(', \n')
        ok = 1
        if(len(fn) == 0):
            continue
        if(onlyIfNotSet):
            if fn in globals().keys():
                # override empty lists, dict, dataframe and items
                if isinstance(globals()[fn], type):
                    continue
                elif isinstance(globals()[fn], pd.DataFrame):
                    if globals()[fn].shape[0] > 0:
                        continue
                elif(isinstance(globals()[fn], dict)):
                    if(len(globals()[fn]) > 0):
                        continue
                elif(isinstance(globals()[fn], list)):
                    if(len(globals()[fn]) > 0):
                        continue
                elif(isinstance(globals()[fn], scipy.sparse.csr.csr_matrix)):
                    if(globals()[fn].shape[0] > 0):
                        continue
                elif(isinstance(globals()[fn], np.ndarray)):
                    if(globals()[fn].shape[0] > 0):
                        continue
    # si dÃ©jÃ  dÃ©finie, passer au prochain
                elif(globals()[fn]):
                    continue
        globals().update(alpow.resume(fn))
    # endfor fn
    return

def save2(
  exc=[],
  fn='allVars',
  include=False,
  backup=False,
  ftp=True,
  cleanup=False,
  zip=True,
  authTypes=[
      str,
      dict,
      list,
      int,
      np.ndarray,
      pd.DataFrame,
      pd.Series]):
  if (not GG('useFTP')) | (GG('sftp')['h'] == '-'):
      p('ftp offline')
      return

  global ftplist
  if(isinstance(exc, str)):  # quicksave single var
      excs = exc.split(',')
      for exc in excs:
          exc = exc.strip(', \n')
          if(len(exc) == 0):
              continue
          fn = exc
          include = [exc]
          exc = []
          alpow._save(
              globals(),
              exclusions=exc,
              fn=fn,
              include=include,
              backup=backup,
              ftp=ftp,
              cleanup=cleanup,
              zip=zip,
              authTypes=False)
      # p(excs)
      return 1
  elif exc == []:
      exc = exclusions
  alpow._save(
      globals(),
      exclusions=exc,
      fn=fn,
      include=include,
      backup=backup,
      ftp=ftp,
      cleanup=cleanup,
      zip=zip,
      authTypes=authTypes)
exclusions=['dnotknow-allwaysExlucdeThisVarFromBeingSaved']

import collections
def asort(dict):
  if type(dict) == list:
    return dict  # allready
  return sorted(dict.items(), key=operator.itemgetter(1), reverse=False)
  #return collections.OrderedDict({k: v for k, v in sorted(dict.items(), key=lambda item: item[1])} )        

def arsort(dict):
  if type(dict) == list:
    return dict  # allready        
  return sorted(dict.items(), key=operator.itemgetter(1), reverse=True)
  #return collections.OrderedDict({k: v for k, v in sorted(dict.items(), key=lambda item: item[1], reverse=True)} )        

p(date())

"""---
###GridSearchCV
---
"""

#gridsearchcv
def modelsEvalutation(variableAPredireNonPresente=False,reset=False,multiplyPredicitions=False,matchAgainst=''):  
  global bss,grid_search,bestNP,votedm,dfNewKeys,dfNoEnergies,models,voting_grid,energy,dfn,ev,fi,pred,x1,x2,y1,y2,mean,bestParameters,voting,r2s,r2,k1,voted,splittedData,resultsArray,y_means,resultss;#ev,r2,msqrt,grid_search,k2,acy,pred;
  bestNP=[];bss={}
  #ev={};fi={};pred={};r2s={};#reset those indices  
  energiess=toPredict
  #noPG or _ignoreParameters
  if(variableAPredireNonPresente):
    energiess=[variableAPredireNonPresente]

  for energy in energiess:
    toGuess=index=energy;    
    print('_'*180)
    print('}'+energy+'{')
    #_df=labelEncoded.copy()
    
    if(variableAPredireNonPresente):
      results=dfNewKeys[variableAPredireNonPresente].fillna(0).astype('float')
    elif energy not in labEncCols:
      results=dfNewKeys[energy].fillna(0).astype('float')  
      #_df.insert(2,energy,dfNewKeys[energy].values,True) 
    else:
      results=labelEncoded[energy].fillna(0).astype('float')      

    print(results[:2])
    mean=results.mean()

    resultss[energy]=results
    y_means[energy]=mean;    
    
    dfs={'trimmed':dfNoEnergies}
    if(includeOriginalDf):      
      dfs['original']=labelEncoded.drop([energy],axis=1)
    #RÃ©-incorporation
    if True:
      if(includeEnergyStar):
        dfs['EnergyStar']=dfNoEnergies.copy(deep=True)
        dfs['EnergyStar']['ENERGYSTARScore']=labelEncoded['ENERGYSTARScore']          

    if 'voting' in models.keys():
      del models['voting'];
  #KeyError: 'Electricity(kBtu)-trimmed'SSSSS
    for dfn in dfs:  
      k1=energy+'-'+dfn;
      if(k1 not in r2s.keys()):
        r2s[k1]={}
      if(reset):
        r2s[k1]={}
        
      print('_'*190)
      print('}'+k1+'{')
      dfx=dfs[dfn]
      #history[dfn+'-'+energy]={}           
      if dfn+'-'+energy in splittedData.keys():
        print('Keeping previous splitted data')
        x1, y1, x2, y2, mean = splittedData[dfn+'-'+energy]#todo:laisser en alÃ©atoire : rÃ©pÃ©tant 20 fois, si 2 coups succeffis
      else:
        x1, y1, x2, y2 = ShuffleOrNot(dfx,results)#Stays the same !
        splittedData[dfn+'-'+energy]=[x1, y1, x2, y2,mean];
        resultsArray[dfn+'-'+energy]=y2
      #x1, y1, x2, y2 = sklearn.model_selection.train_test_split(dfx,results,train_size = 0.8, test_size = 0.2, random_state=100 )
      a=time();
      votingModels=[];  
      votedm=[];
      voting_grid={}    

      #}foreach model{    
      for model in models.keys():      
        k2=k1+'-'+model        
        if (voted==1) & (model != 'dummy'):#only voted dummy is cheating !
          votedm+=[model]  
          votingModels.append((model,models[model]))#later
          if model in param_grid.keys():
            for p in param_grid[model]:
              voting_grid[model+'__'+p]=param_grid[model][p]#pas gÃ©nial ==> trop de candidates
  #'dummy__strategy':['mean','median'],          
  ##for i in param_grid['model']=voting_grid['model__param']= get All Parameters !          
          continue;
        #individual Metrics Here !
        print('\n\t}'+k2+'{\n')
        print('multiplyPredicitions',multiplyPredicitions)

        gsm(models,model,k1,multiplyPredicitions=multiplyPredicitions,matchAgainst=matchAgainst)

        if(len(grid_search.best_params_)):
          print('\tBest Parameters :: ',end='');
          print(len(grid_search.best_params_),end='')
          print(grid_search.best_params_)
          if ignoreParameters:
            print('multiplyPredicitions',multiplyPredicitions);
            gsm(models,model,k1,1,multiplyPredicitions=multiplyPredicitions,matchAgainst=matchAgainst)
        else:
          print('\t no parameters..')
          #.r2_score(y2,predictions)   #.predict(y1)
        #r22=sklearn.metrics.r2_score(y2,grid_search.best_estimator_.predict(y1))#revient Ã  faire exactement la mÃªme chose que prÃ©cÃ©dement..
        r22=grid_search.best_score_
        bss[model]=bs=r22;#grid_search.best_score_
        print('\tBS:'+str(bs))
      #}endformodel{

      if 'voting' in r2s[k1].keys():
        del r2s[k1]['voting'];#previous run
      
      print('\n\t\t ==> Best Reel Model r2:',end='')
      print(arsort(r2s[k1])[:1])
      print('\t\t ==> Best Reel Cv Score:',end='')
      print(arsort(bss)[:1])    
  #colect parameters for voting !      
      if voted==2:
        modelOk=0
  #voted=1 blind vote on cv results
        if(votingOnReel):
          topModels=arsort(r2s[k1])
        else:
          topModels=arsort(bss)
        for mdl,r2score in topModels:
          if(mdl in['dummy','voting']):#cheating, no recursion
            continue
          #from another run, subset
          if mdl not in models.keys():
            continue
          if modelOk>nbBestModel4Voting:
            break;
  #ne pas prendre les coeff nÃ©gatifs ( moins que dummy )           
          if(votingOnReel & (r2score<-0.05)):
            continue
          votingModels.append((mdl,models[mdl]))#selected
          modelOk+=1
          k2=k1+'-'+mdl
          votedm+=[mdl]     
          if k2 in bestParameters.keys():
            for param in bestParameters[k2]:
              #print(param,bestParameters[k2][param])
              voting_grid[mdl+'__'+param]=[bestParameters[k2][param]]    

  #todo: get best 3 models, and their parameters as well .. bestParameters[k2] for voting ==> voting_grid
      #arsort(r2s[k1])[:3] 
      if voted==0:
        print('no voting')
        continue;

      if len(votingModels)==0:
        print('no suitable voting models')
        continue
  #Ensemble methods
      model='voting';k2=energy+'-'+dfn+'-'+model
      print('\n\nvoting between : '+','.join(votedm))
      print(voting_grid)
  #TypeError: VotingRegressor __init__() got an unexpected keyword argument 'scoring'  
      models[model]=sklearn.ensemble.VotingRegressor(votingModels)#,scoring=scoring,, voting='soft'
  #todo:bagging here      
      gsm(models,model,k1,multiplyPredicitions=multiplyPredicitions,matchAgainst=matchAgainst)
      #pour prochains passages
      del models['voting']
      print('\n')
      #print('voting classifier : best :');print(grid_search.best_estimator_);#is voting
      #grid_search.best_estimator_.estimators
      #score(self, X, y[, sample_weight]);Return the coefficient of determination R^2 of the prediction.        
      if(len(r2s[k1])):
        print('\t\t ==> best model r2 with voting is :',end='')
        print(arsort(r2s[k1])[:1])  
      if(len(bss)):
        print('Best CV score')
        best=arsort(bss)[:1][0];print(best)     
      #none#print('\t\t ==> best model cv score is :',end='');print(arsort(bss[k1])[:1])
      print('\t\tVoting Classifier final r2:'+str(r2));                 
      message('all models processed')
      save(globals(),[],'pendingModels-'+filename(energy),'energy,dfn,r2s,pred,resultsArray,y_means'.split(','));#
    #}endfor dataframe{
    if(stopAtOne):
      return;
  message(energy+' processed')
  save(globals(),[],'magicModelsRunnedWithVotingOnAllEnergiesShort-'+filename(energy),'r2s,pred,resultsArray,y_means'.split(','));#
  #}endfor energy{  
  return 1;

def gsm(__models,model,k1,_ignoreParameters=0,abort=False,multiplyPredicitions=False,matchAgainst=''):
  #globales pour Ã©critures,autonome en lecture
  global lastVotingKey,mean,y2,grid_search,bestNP,previousR2,votedm,models,ev,fi,pred,_df,y_mean,per,x,voting,r2,voting_grid,cvResults,bestParameters,k2;#,x1,x2,y1,y2energy,dfn,

  #rÃ©cupÃ¨re sur scope global quand non spÃ©cifiÃ© mais Ã©crit au niveau de la fonction quand non global !
  #global ev,r2,msqrt,grid_search,k2,acy,pred;
  #global energy,dfn,grid_search,mean,param_grid,x1,x2,y1,y1,pred,ev,toGuess,acy,t,a,nbFold,scoring,arretAnticipe,history,bp,bestParameters
  k=model;
  suffix=''
  k2=k1+'-'+model
  p('multiplyPredicitions',multiplyPredicitions)

  if multiplyPredicitions:
    k2+='-M:'+str(multiplyPredicitions)+'-A:'+matchAgainst

  if(model not in param_grid.keys()):
    param_grid[model]={}
    p(model,'no within',param_grid)

  _pg=param_grid[model]

  if model=='voting':
    _pg=voting_grid
    suffix+='-'+','.join(votedm)
    k2+='-'+','.join(votedm)
    lastVotingKey=k2    

  if((_ignoreParameters == 1) or (noPG)):
    suffix+='-np';
    k2+='-np'
    _pg={}

  if(k2 not in ev.keys()):
    ev[k2]={}
      
  #params = {'modelName__Param':[],'logisticregression__C': [1.0, 100.0],'randomforestclassifier__n_estimators': [20, 200],}  
  #ValueError: Parameter values for parameter (MLPRegressor) need to be a sequence(but not a string) or np.ndarray.    
  modelHasRandomState = model in hasRandomState
  if model.startswith('Ridge'):
    modelHasRandomState=True

  if modelHasRandomState & ('random_state' not in _pg):
    _randomKeys=nbRdKeys
    if(abort):
      pass
    elif(model in nbRdKeysPerModel.keys()):
      _randomKeys=nbRdKeysPerModel[model]
    #print(i+' hasRandomState')   
    _pg['random_state']=range(1,_randomKeys);#always produce similar results, mais en permettant 3 shuffle, une fois les meilleurs paramÃ¨tres retenus pour voting :):)    
    print(_pg['random_state'])
  #signature=k2+str(hash(json.dumps(_pg)))  

  print('\n'+model+suffix+' :: param grid :: ',end='');print(_pg)
  
  if(False):# & (signature in repeatedGridSearchCVSameResults)
    print('grid_search allready exists')
    grid_search=repeatedGridSearchCVSameResults[signature]
  else:
    #print(signature);repeatedGridSearchCVSameResults[signature]=
    grid_search=sklearn.model_selection.GridSearchCV(models[model], _pg, cv=my_cv, n_jobs = -1, scoring=scoring, refit=True, verbose=1, return_train_score=True)  #
  
  a=time();
  #when model is a classifier .. fit Transform the Results !
  if(k=='LogisticRegression'):
    x3=sklearn.preprocessing.LabelEncoder().fit_transform(x2)
    grid_search.fit(x1,x3)
  else:
    #here ..
    grid_search.fit(x1,x2)
  #now its okay   
  if modelHasRandomState: 
    del _pg['random_state']
  #Report Best Parameters MSE for the Grid Search !!!
  #sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=False, random_state=None)
  #CV = KFold(n_splits=5, random_state=None, shuffle=False)
  #cv_results_  
  cvResults[k2]=grid_search.cv_results_
  bestParameters[k2]=grid_search.best_params_
  if 'random_state' in grid_search.best_params_:
    rds=grid_search.best_params_['random_state']
    if rds not in bestRandomStateValues.keys():
      bestRandomStateValues[rds]=0
    bestRandomStateValues[rds]+=1

  if(multiplyPredicitions):
    print('\n\t\tpredictions multiplied by : '+multiplyPredicitions)
    pred[k2]=predictions=grid_search.predict(y1)*y1[multiplyPredicitions]
  else:
    pred[k2]=predictions=grid_search.predict(y1)

  predMean=predictions.mean()#0 = bof bof guassian  
  #KneighValueError: Expected 2D array, got 1D array instead:array=[ 36  72 134 ...  38 254  25].Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
  #score=grid_search.score(y2,predictions)#scoring selected#is not a serie ...  
  #print(ev)print(ev.keys())
  #assert(False)
  if True:
    bs=grid_search.best_score_
    if(matchAgainst):
      print('\t\tmatchedAgainst:'+matchAgainst)
      results=labelEncoded[matchAgainst].fillna(0).astype('float')    
      if dfn+'-'+matchAgainst in splittedData.keys():
        print('Keeping previous splitted data')
        _x1, _y1, _x2, y2, mean = splittedData[dfn+'-'+matchAgainst]
      else:
        print('Generating new split')
        _x1, _y1, _x2, y2 = ShuffleOrNot(dfx,results)#Stays the same !
    #r2=sklearn.metrics.r2_score(Kbtu_y2,predictions)#matchAgainst labenc='Electricity(kBtu)'   

    r2s[k1][model+suffix]=ev[k2]['r2']=r2=sklearn.metrics.r2_score(y2,predictions)   #.predict(y1) 
    if abort:
      print('Real rd:'+str(round(r2,2))+',bs:'+str(bs));
      return grid_search;

    nmse=sklearn.metrics.mean_squared_error(y2,predictions)
    sqrt=ev[k2]['sqrt']=round(nmse ** (1/2))#=math.sqrt(nmse) mean_squared_log_error
    ev[k2]['bs']=nmse
    ev[k2]['nmse']=nmse    
    ev[k2]['mean']=round(mean,2)
    ev[k2]['PredMean']=round(predMean,2)
    ev[k2]['time']=round(time()-a,1)
    ev[k2]['explained_variance_score']=sklearn.metrics.explained_variance_score(y2,predictions)    
    per=ev[k2]['per']=round(sqrt/mean,5)

  sep='+';
  if(r2<0):
    sep='';

  #bestparams[toGuess+k]=grid_search.best_params_
  #AttributeError: 'KNeighborsClassifier' object has no attribute 'feature_importances_'
  #grid_search.best_estimator_.feature_importances_
  if hasattr(grid_search,'best_estimator_'):#IS VOTING !
    if hasattr(grid_search.best_estimator_, 'feature_importances_'):
      fi[k2]=grid_search.best_estimator_.feature_importances_
      plot_feature_importances(grid_search.best_estimator_.feature_importances_,dfn+'-'+energy+'-'+model, x1.columns.values,fn='FeatImportance-'+filename(k2)+str(round(r2,2))+'.png')

  elif hasattr(grid_search,'feature_importances_'):      
    fi[k2]=grid_search.feature_importances_
    plot_feature_importances(grid_search.feature_importances_,dfn+'-'+energy+'-'+model, x1.columns.values,fn='FeatImportance-'+filename(k2)+str(round(r2,2))+'.png')

  plt.rcParams["figure.figsize"] = (24,12)
  #plt.rcParams["figure.figsize"] = (6,6);plt.scatter(y2,predictions,alpha=0.2)
  y_mean = [mean]*len(y2)
  _df=pd.DataFrame(list(zip(y2,predictions,y_mean)),columns=['Real','Guess','Mean'])

  #fig,ax = plt.subplots()
  fn=filename(k2)
  if(len(fn)>100):
    fn=fn[0:50]
  fn='sp-'+fn+'.r2:'+sep+str(round(r2,2))+'.p:'+str(round(per,2))+'.png'
  f=_df[['Real','Guess','Mean']].plot(alpha=0.4,style=['o','r^','g-'],title='DonnÃ©es RÃ©elles vs prÃ©dictions '+dfn+'-'+energy+'-'+model+suffix+' r2:'+str(round(r2,2))+','+str(round(per,2))+'%');
  #f.getAxes;
  #f.plot(y2.index,y_mean, label='Mean', linestyle='--',color=[0,0.5,0,1],alpha=1)
  plt.yscale('log');f.get_figure().savefig(fn, bbox_inches = 'tight',dpi=dpi);webp(fn);#show();

  print('\n\t',end='');print(ev[k2])
  if _ignoreParameters:
    if r2>=previousR2:
      bestNP+=[model]
      print('\n\n\t\t==> Best results without parameters')

  previousR2=r2
  return grid_search;  
p(date())

"""---
# Cross-Validation & random_state
---
- Stake of cross-validation would be to return same determination coefficients "<b>RÂ²</b>"  between two runs :: always pass a fixed random_state: 42 to grid_params in the function which will pass to grid_search_cv, however this can protect us from better results .. Meanwhile .. running several runs with various keys ( <u>please store them</u> ) might get you better results, not based on the grid, but based on the starting point randomness wich might lead you to a best "local" function loss
---
- Just imagine this map, wider, with more hole, across several dimensions, and you'll get your main picture, but somehow, just don't waste that much time with randomness, cause you'll train an overspecialized model .. and that's not the purpose here ..
<img src='https://snipboard.io/QpSUvP.jpg'/>
---
- In order to be able to compare the predictions, it is necessary to operate each separation of the data set in training, test, in a non-random manner, in order to be able to reproduce the results
- <u> Random_state </u> The same goes for the results of certain regression models, the cross validation taking into account a hyper-parameter matrix must in no case return a random state, which would distort all interpretation
- We cannot select a "best" model based on the scoring of the latter with respect to the data to be predicted, since we are not supposed to have this information, this data may vary in the future
- The goal of cross-validation is to lessen "over-fitting" a model so that it reacts better in a global way to the presentation of new input data

---
## Load data
---
"""

ftpls();#GG('ftplist')#list files from ftp, so they're available for downloading ..
getFile('2015-building-energy-benchmarking.csv')#gets the zip & unzip if exists
getFile('2016-building-energy-benchmarking.csv')

df2015=loadData('2015-building-energy-benchmarking.csv')#Total Rows in dataset :156980,Rows containing null values :106880
df2016=loadData('2016-building-energy-benchmarking.csv')#Total Rows in dataset :155296,Rows containing null values :87776

indf2015Only=set(df2015.columns) - set(df2016.columns)#{'2010 Census Tracts','City Council Districts','Comment','GHGEmissions(MetricTonsCO2e)','GHGEmissionsIntensity(kgCO2e/ft2)','Location','OtherFuelUse(kBtu)','SPD Beats','Seattle Police Department Micro Community Policing Plan Areas','Zip Codes'}
indf2016Only=set(df2016.columns) - set(df2015.columns)#{'Address','City','Comments','GHGEmissionsIntensity','Latitude','Longitude','State','TotalGHGEmissions','ZipCode'}
#df2015['Year']=2015;df2016['Year']=2016;#DataYear exists
print('nbcol 2015:'+str(len(df2015.columns))+',nbrows:'+str(df2015.shape[0]))#47
print('nbcol 2016:'+str(df2016.shape[1])+',nbrows:'+str(df2016.shape[0]))#46

print('Common columns within both files :'+str(len(frozenset(df2015.columns).intersection(df2016.columns))))#37 en commun
print('Data in 2015 only:\n\t\t'+','.join(indf2015Only))
print('Data in 2016 only:\n\t\t'+','.join(indf2016Only))
p(date())

"""#.-------------------------------------------------------------

---
# II) Different dataframes
- Some of these columns contain the same data, but do not have the same names ..
- {'2010 Census Tracts','City Council Districts','Comment','GHGEmissions(MetricTonsCO2e)','GHGEmissionsIntensity(kgCO2e/ft2)','Location','OtherFuelUse(kBtu)','SPD Beats','Seattle Police Department Micro Community Policing Plan Areas','Zip Codes'} vs {'Address','City','Comments','GHGEmissionsIntensity','Latitude','Longitude','State','TotalGHGEmissions','ZipCode'}
----
"""

print(',\t'.join(df2015.columns))
print(',\t'.join(df2016.columns))
print(df2015['Location'][:2][0])
print(df2016['Address'][:2])
p(date())
#df2016['TotalGHGEmissions']=df2015['GHGEmissions(MetricTonsCO2e)']#?

"""---
## Location df2015 -> json{Lat,Lon}
- Location est un champ Json rÃ©parti sur les nouvelles colonnes du fichier 2016 on constate rapidement que ces donnÃ©es concernent les mÃªmes batiments aux mÃªmes adresses et c'est mieux
- GHGEmissionsIntensity<=GHGEmissionsIntensity(kgCO2e/ft2)
---
"""

### Le champ location a l'air d'Ãªtre un json qui mÃ©rite d'Ãªtre rÃ©parti entre plusieurs colonnes afin d'avoir de belles corrÃ©lations
results = pd.DataFrame()
for idx, row in df2015.iterrows():
    loc = ast.literal_eval(row['Location'])
    row = row.drop('Location')
    lat = loc['latitude']
    lon = loc['longitude']
    need_recoding = 0
    if('needs_recoding' in loc.keys()):
        need_recoding = loc['needs_recoding'] 
#'{\'latitude\': \'47.61219025\', \'human_address\': \'{"address": "405 OLIVE WAY", "city": "SEATTLE", "state": "WA", "zip": "98101"}\', \'longitude\': \'-122.33799744\'}'        
    normalize = pd.Series(json.loads(loc['human_address']))
    
    cols = list(row.index) + ['latitude', 'longitude', 'need_recoding'] + list(normalize.index)
    x = pd.DataFrame([list(row) + [lat, lon, need_recoding] + list(normalize)], columns = cols )
    results = results.append(x).reset_index(drop=True)

results['longitude']=results['longitude'].astype(float)
results['latitude']=results['latitude'].astype(float)
df2015=results
del results
p(date())

"""---
## Harmonisation colonnes
- Harmonizons les noms de colonnes entre les deux datasets
---
"""

df2016['address']=df2016['Address']
df2016['city']=df2016['City']
df2016['state']=df2016['State']
df2016['zip']=df2016['ZipCode']#Zip Codes
df2016['latitude']=df2016['Latitude']
df2016['longitude']=df2016['Longitude']
df2016['Comment']=df2016['Comments']
df2016['GHGEmissionsIntensity(kgCO2e/ft2)']=df2016['GHGEmissionsIntensity']### ou multiplier celle ci par superficie et tester si rÃ©sultat pertinent
df2016['GHGEmissions(MetricTonsCO2e)']=df2016['TotalGHGEmissions']#### this one
df2016=df2016.drop(['Address','City','State','ZipCode','Latitude','Longitude','Comments','GHGEmissionsIntensity','TotalGHGEmissions'],axis=1)
p(date())

"""---
## OK ?
- Reste t-il des colonnes uniques Ã  2016 ??? Si oui l'execution s'interrompt !
---
"""

indf2016Only=set(df2016.columns) - set(df2015.columns)#
if(len(indf2016Only)>0):
  print('DonnÃ©es prÃ©sentes dans 2016 uniquement:\n\t\t'+','.join(indf2016Only))#
  assert(False)
p(date())

"""---
### Merge & Clean
- Valeurs non remplies : potentiellement Ã  supprimer, notamment pour les relevÃ©s Ã  prÃ©dire : 
- OtherFuelUse(kBtu) => empty 6699 99.75%
---
- SteamUse(kBtu) => empty : 6456 96.13%
- ENERGYSTARScore => empty : 1623 24.17%
---
"""

def cleanData(inputDf,fillStrings='na',fillInt=0,considerEmpty=[np.inf,-np.inf,np.nan,0,'na','']): 
  #test = pd.read_csv('../input/test.csv')
  df=inputDf.copy(deep=True)
  rows=df.shape[0]
  cols=df.shape[1]  
  dfs=df.size;

  nanInfZeroNaEmpty=train.isin(considerEmpty).sum().sort_values(ascending=False)  
  nv=nanInfZeroNaEmpty.sum()

  print('Total Rows in dataset : '+str(rows));
  print('Total Cols in dataset : '+str(cols));
  print('Total Cells : '+str(dfs))
  print('Cells containing null,inf,NaN,0 or empty values : '+str(nv)+' ( '+str(round(nv*100/dfs,2))+'% )');#Diagnose null columns
  print('_'*80)  
  # Fournissent une bonne indication des colonnes Ã  dropper pour le modÃ¨ke
  for i in nanInfZeroNaEmpty.index:
    nbempty=nanInfZeroNaEmpty[i]
    per=round(nbempty*100/rows,2);
    print(i+' => empty : '+str(nbempty)+' '+str(per)+'%')

  print('_'*140)
  
  for i in df.columns:
    if df.dtypes[i]==object:
      df[i].fillna(fillStrings,inplace=True)#as strings ! boljemoi !
    else:
      df.replace([np.inf,-np.inf,np.nan],fillInt,inplace=True)#replace infinite by Nan
  #df.fillna(df.mean(),inplace=True)#Numeric types corrected
  #df.fillna(fillInt,inplace=True)#2 rows of Nan -> is valid either for strings or numeric datatype
  #dtype as str please !!
  
  print('_'*140)
  print('Cells with null values then : '+str(nullValues(df).size))#CAUTION !!! WONT FIT INTO MODEL OTHERWISE
  print('_'*140)
  return df;

train=df2015.append(df2016)
train['zip']=train['zip'].fillna(0).astype('int')
train=cleanData(train)
deli('cleaned,df2016,df2015')
print(','.join(train.columns))

"""---
## Describe
- On ne retient que les valeurs uniques par type de colonne: string, on constate rapidement que : location et propertyname ont bcp de valeurs uniques qu'il sera appropriÃ© d'abandonner avant de les insÃ©rer dans le modÃ¨le ( les autres gÃ©nÃ©ront des colonnes boolÃ©enne pour chaucune de leurs valeurs respectives si la corrÃ©lation est suffisante pour ces derniÃ¨res )
- ListOfAllPropertyUseTypes: contient tous les usages d'un batiment avec des valeurs sÃ©parÃ©es par des virgules
---
"""

p(','.join(train.columns))
train.describe()

"""---
### Unique Values ? 
- Nombreuses valeurs distinctes ( pour les strings ) => Risque de sur-apprentissage !
- TaxParcelIdentificationNumber;PropertyName;Comment;YearsENERGYSTARCertified;address;city;state
---
"""

unikValuesPerDataframe(train)

"""---
## >Latlon
- Arrondir Ã  3 dÃ©cimales: 100 mÃ¨tres prÃ¨s et crÃ©er une clÃ© composite, cela peut - il servir Ã  regrouper, effectuer des corrÃ©lations gÃ©ographiques ?
---
"""

train['shortlat']=train['latitude'].round(2)#3:110mÃ¨tre prÃ¨s,2: 1 km
train['shortlon']=train['longitude'].round(2)
train['shortlatlon']=train['shortlat'].astype('str')+','+train['shortlon'].astype('str')

plt.rcParams["figure.figsize"] = (24,24)
print('Here is the form Elliott Bay of the left')#,title='EliottBay'
scatter(train,'longitude','latitude',fn='eliottBayLatLonPrint.png')#
plt.rcParams["figure.figsize"] = (22,6)
plt.scatter(train.index,train['latitude']);plt.title('latitude');plt.savefig('lat.png', bbox_inches = 'tight',dpi=dpi);webp('lat.png');#show();
plt.scatter(train.index,train['longitude']);plt.title('longitude');plt.savefig('lon.png', bbox_inches = 'tight',dpi=dpi);webp('lon.png');#show();
#Needs X Avis to be numeric
#train.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4)
#https://www.bigendiandata.com/2017-06-27-Mapping_in_Jupyter/
#save2()#all variables as one zip to be restored via function :: load2()
#save(globals(),'df2015,df2016,train'.split(','))#globals().update(resume('shortlatlon')),df,results,x1,y1,correlations,x2,y2,row,heatmap,topCorrelations

"""---
### Ö Interesting Metrics > KDE
- > Suppress : OtherFuelUse(kBtu) : filled at 0.25%
- http://1.x24.fr/a/jupyter/seattle/kde2.webp
---
"""

interestingMetrics='ENERGYSTARScore,Electricity(kBtu),GHGEmissions(MetricTonsCO2e),SiteEUIWN(kBtu/sf),SourceEUIWN(kBtu/sf),shortlat,shortlon,SiteEnergyUse(kBtu),GHGEmissionsIntensity(kgCO2e/ft2),OtherFuelUse(kBtu),NaturalGas(kBtu),SteamUse(kBtu)'.split(',')
rows=train.shape[0]
completion={}
for i in interestingMetrics:
  completion[i]=round(train[train[i]!=0].shape[0]*100/rows,2)

print(asort(completion))
interestingMetrics.remove('OtherFuelUse(kBtu)')#0.25%

lowQuantileLimit=0.01
highQuantileLimit=0.99
#shortlatlon is not numeric ! train['shortlatlon'] 
plt.rcParams["figure.figsize"] = (24,36)
heightPerGraph=widthPerGraph=10
j=0;axx=2;#Per Col
axy=math.ceil(len(interestingMetrics)/axx);#tjrs 1 de plus
print(str(axx*axy)+' tot slots for '+str(len(interestingMetrics))+' to display : ok')#=) provisionner plus d'emplacements n'est pas grave
f,axes=plt.subplots(axy,axx,figsize=(axy*heightPerGraph,axx*widthPerGraph))
f.patch.set_facecolor('white');axes=axes.flatten()#pour les parcourir plus facilement sur une seule dimension :)

for i in interestingMetrics:  
  #print(i)
  x=train[train[i]!=0][i]
  if x.shape[0]==0:
    continue;
  ax=axes[j]    
  minx=train[i].quantile(lowQuantileLimit);
  maxx=train[i].quantile(highQuantileLimit);
  ax.set_xlim(minx,maxx)#Avec les limites adaptÃ©es Ã  chaque type de donnÃ©es  
  x.plot(kind='kde',ax=ax,title=i);
  j=j+1;#prochain axe

fn='kde2.png';f.savefig(fn, bbox_inches = 'tight',dpi=dpi);webp(fn);#show()

"""---
## Feature Engineering : Electric Consumption per GFA ?
- Certaines colonnes numÃ©riques sont des totaux : metrictons, Kwh
- D'autres sont reportÃ©es / sqft => Il est intÃ©ressant de reporter certaines mÃ©triques absolues Ã  la surface du batiment afin de matcher d'autres variables
---
- Utile pour rapporter les mÃ©triques totales Ã  une surface afin de mieux corrÃ©ler certaines variables
=> Kbtu => Kbtu/sqft
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-ghgemissions-mt-gfa-ghgemissionsintensity-kgco2e-ft2--1.0.webp#Validation du POC
- http://1.x24.fr/a/jupyter/seattle/correlationsInterestingMetricsAndComposite.webp Meilleures corrÃ©lations pour ElectricitÃ© etc ..
---
"""

#train[['PropertyGFATotal','PropertyGFABuilding(s)']][:10]#
train['Electricity-KBtu/Gfa']=train['Electricity(kBtu)']/train['PropertyGFATotal']
train['SiteEnergyUseWN-KBtu/Gfa']=train['SiteEnergyUseWN(kBtu)']/train['PropertyGFATotal']
train['Steam-KBtu/Gfa']=train['SteamUse(kBtu)']/train['PropertyGFATotal']
train['GHGEmissions-Mt/Gfa']=train['GHGEmissions(MetricTonsCO2e)']/train['PropertyGFATotal']
train['NaturalGas-KBtu/Gfa']=train['NaturalGas(kBtu)']/train['PropertyGFATotal']
#train['OtherFuelUse-kBtu/Gfa']=train['OtherFuelUse(kBtu)']/train['PropertyGFATotal']##,OtherFuelUse-kBtu/Gfa
#on les ajoute naturellement
newKeys='Electricity-KBtu/Gfa,SiteEnergyUseWN-KBtu/Gfa,Steam-KBtu/Gfa,GHGEmissions-Mt/Gfa,NaturalGas-KBtu/Gfa'.split(',')

"""---
### ! Dummies for Properties
- Obtenir des valeurs uniques pour chaque dÃ©limitation par virgule
"""

#del train;resumed=0;globals().update(resume('shortlatlon'))
dummies={}
results = pd.DataFrame()
colName='ListOfAllPropertyUseTypes';
train[colName].fillna('',inplace=True)#
train[colName]=train[colName].astype('str')

for idx, row in train.iterrows():
  x=re.sub(r"[^a-z0-9, \.\-_:]+",'-', row[colName].lower());
  #row=row.drop(colName)#does nothing ?
  if(type(x)==int):
    continue;

  r=x.split(',')
  nc=list(map(str.strip,r));#strip spaces
  nc=['z_'+colName+'_'+format(j) for j in nc]
  newData=[];
#  for i in range(len(nc)):
#    newData=newData+[1];
  for i in nc:    
    if i not in dummies:
      dummies[i]=0;
    dummies[i]+=1;
    newData=newData+[1];

  data = list(row) + newData;#DonnÃ©es anciennes + nouvelles
  cols = list(row.index) + nc;#len(cols)#53
  ndf = pd.DataFrame([data], columns=cols)
#AssertionError: Number of manager items must equal union of block items
# manager items: 607, # tot_items: 608  
  results = results.append(ndf).reset_index(drop=True)
    
results.fillna(0,inplace=True)#replace na values foreach new column
FPCP('train', train)
train=results;del results;#replace dataframe with the new one
FPCP('results', train)
#train[:1]
p('TopDummies',arsort(dummies)[:5]);

if False:
  newColumns=[]
  for i in train['ListOfAllPropertyUseTypes'].unique():    
      r=i.replace("'",'').split(',')
      r=list(map(str.strip,r));
      newColumns=list(set(newColumns+r));
  #print(Counter(newColumns))
  len(newColumns)#70 out of 443 unique concatened values
  #print();break;
dummiesk=list(dummies.keys());  
p(','.join(dummiesk))
save(globals(),'df,results,df2015,df2016,x1,y1,correlations,x2,y2,row,heatmap,topCorrelations'.split(','),'dummies')#globals().update(resume('dummies'))#dummies checkpoiunt

"""---
## Drop Btu/Kw/Therms Unit Duplicates
- Certaines variables sont en double ( en comportant des unitÃ©s diffÃ©rentes qui vons forcÃ©ment corrÃ©ler Ã  100%
"""

dropped=[];#Conserver que Kbtu ( majeure partie du dataframe )
#Retirer les corrÃ©lations "Ã©videntes" crÃ©es entre des champs ayant peu de valeurs ou bcp trop en commun    
toDrop='DataYear;OSEBuildingID;TaxParcelIdentificationNumber;PropertyName;Comment;YearsENERGYSTARCertified;address;city;state'.split(';')
for i in train.columns:
  if('therms' in  i or 'kWh' in i or i in toDrop):    
    dropped+=[i]

train=train.drop(dropped,axis=1)
save(globals(),'df,results,df2015,df2016,x1,y1,correlations,x2,y2,row,heatmap'.split(','),'dropedColumns')#globals().update(resume('dropedColumns'))
p('Dropped Columns :')
p(dropped)
#RFM,R,Frequency,avgCartPrice,avgItemPrice

"""---
## CorrÃ©lations
- TOP Des corrÃ©lations, le changement d'unitÃ© de mesure est une Ã©vidence Ã  exclure ( ne conserver qu'une unitÃ© de mesure pour chaque )
- Nb : les corrÃ©lations nÃ©gatives sont des corrÃ©lations : par exemple plus la batiment a des habitats multifamilliaux et plus sa consommation Ã©lectrique globale diminue, ici nous allons appliquer une valeur absolue afin de mieux pouvoir classifier ces derniÃ¨res
---
- Quincailleries, cordonniers, certains commerces possÃ¨dent des virgules dans leurs descriptifs pour dÃ©crire ce qu'ils sont d'oÃ¹ ces corrÃ©lations .. + Features crÃ©es
"""

heatmap=train.copy(deep=True);#   
plt.rcParams["figure.figsize"]=(heatmap.shape[1],heatmap.shape[1])#Massive Output to register to jpeg as much columns * 45px
for i in heatmap.columns.values:
  if 'numpy' not in str(type(heatmap[i][0])): 
    heatmap[i],levels=pd.factorize(heatmap[i])
correlations=heatmap.corr().abs()#Not Absolute We Want to know what might decrease
np.fill_diagonal(correlations.values, np.nan)#no more ones
#exclude z_ ?
topCorrelations=(correlations.where(np.triu(np.ones(correlations.shape),k=1).astype(np.bool)).stack().sort_values(ascending=False))    
pd.set_option('display.max_rows',900)
print(topCorrelations.head(180))

"""---
### Heatmaps
- On constate les Ã©videntes corrÃ©lations entre certains types de commerces prÃ©sents dans une tour, on regroupe les statistiques qui nous intÃ©ressent afin d'en faire un relevÃ© Thermographique avec les autres variables
---
"""

def heatmapx(x,y,fn=0):
  plt.rcParams["figure.figsize"]=(len(x),len(y))
  plt.figure(figsize=(len(x),len(y)));
  if fn==0:
    fn='correlations-'+filename(','.join(y))+'.png'
  sns.heatmap(correlations.loc[y][x],xticklabels=x,yticklabels=y,annot=True,cmap="YlGnBu").get_figure().savefig(fn, bbox_inches = 'tight',dpi=dpi);
  webp(fn);#show() 

c=correlations.shape[0]
im=interestingMetrics.copy()
im.sort()
x=interestingMetrics+newKeys
x.sort()
y=correlations.columns.values

heatmapx(im,im,'correlationsInteresting.png')
heatmapx(y,x,'correlationsGlobales.png')
heatmapx(x,x,'correlationsIFeatures.png')
print('### \t familles, data center, supermarchÃ© consomment bcp electricitÃ© - Meilleures corrÃ©lations avec /ftÂ²')
heatmapx(interestingMetrics+dummiesk,['Electricity(kBtu)','Electricity-KBtu/Gfa'])
heatmapx(interestingMetrics+dummiesk,['SiteEnergyUse(kBtu)','SiteEUIWN(kBtu/sf)','SiteEnergyUseWN-KBtu/Gfa'])
heatmapx(interestingMetrics+dummiesk,['GHGEmissions(MetricTonsCO2e)','GHGEmissionsIntensity(kgCO2e/ft2)', 'GHGEmissions-Mt/Gfa'])
heatmapx(interestingMetrics+dummiesk,['NaturalGas(kBtu)', 'NaturalGas-KBtu/Gfa'])
heatmapx(interestingMetrics+dummiesk,['SteamUse(kBtu)','Steam-KBtu/Gfa'])

heatmapx(interestingMetrics+dummiesk,['ENERGYSTARScore'])#http://1.x24.fr/a/jupyter/seattle/correlations-energystarscore.webp

"""---
### Conclusions : 
- http://1.x24.fr/a/jupyter/seattle/violinPerCatNoLog-electricity-kbtu-gfa-largestpropertyusetype.webp #Violins consommation electrique selon utilisation immeuble
- http://1.x24.fr/a/jupyter/seattle/correlationsGlobales.webp # pour les dummies Ã©galement !
- http://1.x24.fr/a/jupyter/seattle/correlations-electricity-kbtu-electricity-kbtu-gfa.webp #familles, data center, supermarchÃ© consomment bcp electricitÃ© - Meilleures corrÃ©lations avec /ftÂ²
- On peut supprimer les deux en double
----
"""

train=train.drop(['SiteEnergyUseWN-KBtu/Gfa','GHGEmissions-Mt/Gfa'],axis=1)
heatmap=heatmap.drop(['SiteEnergyUseWN-KBtu/Gfa','GHGEmissions-Mt/Gfa'],axis=1)

newKeys.remove('SiteEnergyUseWN-KBtu/Gfa')
newKeys.remove('GHGEmissions-Mt/Gfa')
p(x)
p(dummies.keys())
save(globals(),'df,results'.split(','),'0pairplots')#globals().update(resume('0pairplots'))

"""---
#### ðŸ—² !!!!! Pairplots & Basic RÃ©gressions !
---
"""

heightPerUnit=69336/107/9;#72
le=len(interestingMetrics)-1#2 removed rows
#ne jamais excÃ©der 65536 pixels par dimension ..
maxh=65536/72/le;
graphHeight=8
if(graphHeight>maxh):
  graphHeight=int(maxh)
#Í¶Ñ Ö
x=interestingMetrics.copy()
x+=newKeys
x.sort()
pairs={}

for i in x:  
  cols=interestingMetrics.copy()
  cols+=newKeys
  cols.remove(i)
  cols.sort()
  g=sns.pairplot(heatmap,kind='scatter',height=graphHeight,aspect=1,x_vars=cols,y_vars=[i],plot_kws={'alpha':0.5},markers="+");
  fn='pairplots-'+filename(i)+'.png';
  g.fig.savefig(fn, bbox_inches = 'tight',dpi=dpi);
  webp(fn);#show();  
  #assert(false);

  pairs[i]=[]

  for j in cols:
    if(j in pairs.keys() & i in pairs[j]):
      print('-');
      continue
    pairs[i]+=[j]
    sep='+'
    corrScore=round(correlations[i][j],2);
    if(corrScore<0):
      sep='';
    fn='scatRegressionHeatmap-'+filename(i+'-'+j)+sep+str(corrScore)+'.png'
    snsscat(heatmap,i,j,opacity=0.5,color=[1,0,0.3,0.5],kind='reg',fn=fn,minx=heatmap[i].min(),maxx=heatmap[i].max(),miny=heatmap[j].min(),maxy=heatmap[j].max())#Reg
  #Et des rÃ©gressions / KDE pour chaque facteur ??

"""---
#### EnergyStarScore
- Est construit Ã  partir de ces relevÃ©s : 
 - http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-energystarscore-siteeuiwn-kbtu-sf-+0.3.webp
 - http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-energystarscore-sourceeuiwn-kbtu-sf-+0.27.webp
 - http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-energystarscore-electricity-kbtu-gfa+0.25.webp
 - http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-energystarscore-naturalgas-kbtu-gfa+0.21.webp
---
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-electricity-kbtu--siteenergyuse-kbtu-+0.95.webp #l'Ã©lectricitÃ© est l'Ã©nergie la plus consommÃ©e
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-electricity-kbtu-gfa-ghgemissionsintensity-kgco2e-ft2-+0.43.webp #et a une forte incidence sur l'Ã©mission de gazs Ã  effet de serre .. 
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-naturalgas-kbtu-gfa-ghgemissionsintensity-kgco2e-ft2-+0.9.webp #mais pollue moins que l'usage de le gaz (hydrolien, renouvellable, nuclÃ©aire )
"""

#

"""---
- Pas de corrÃ©lation Lat/Long
- SteamUse : rÃ©seau de chaleur urbaine = gÃ©ant chauffage collectif !
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-siteenergyusewn-kbtu-gfa-siteeuiwn-kbtu-sf--0.93.webp #Plus energivore au mÂ² => SiteEUI 
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-electricity-kbtu-gfa-sourceeuiwn-kbtu-sf--0.92.webp #La production de chauffage sur place augmente la consommation electrique
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-ghgemissions-mt-gfa-naturalgas-kbtu-gfa-0.89.webp # et intrasÃ¨quement chaque consommation Ã©nergÃ©tique augment l'Ã©mission de GHG
----
- 1) Plus un batiment produit une source d'Ã©nergie sur place ( vapeur ) plus il va Ã©mettre de gazs Ã  effet de serre ( chaudiÃ¨re ), l'apport d'Ã©nergies extÃ©rieures aussi, mais dans une moindre importance
- 2) Les corrÃ©lations sont diffÃ©rentes, notamment rapportÃ©es par ftÂ² vu que les unitÃ©s employÃ©es n'ont pas le mÃªme rapport, il faut mieux considÃ©rer la mÃ©trique linÃ©aire pour la plupart
- 3) Mais EUIWN est trÃ¨s cohÃ©rent avec les Ã©mission par mettre carrÃ© ( Energy Use Intensity ) => Energy Star Score est calculÃ© notamment Ã  partir de ces deux variables
- 4) Une centrale Ã©lectrique ou un hopital auront tendance Ã  Ã©mettre plus de gazs Ã  effets de serre que d'autres types de batiments ! 
=> Les batiments de gÃ©nÃ©ration Ã©nergÃ©tique / ou de haute consommation Ã©nergÃ©tique

----
## Best Correlation Keys
- Les longues Heatmap Ã©tant difficiles Ã  lire autant investir dans de simple tableaux
- Certaines corrÃ©lations sont simplement dues Ã  la rÃ©partition statistique des donnÃ©es au sein de leur jeu
- Shortlatlon arrondi Ã  2 dÃ©cimales = 1kmÂ² environ => 42% corrÃ©lation with neighborhood ( quartier ), Ã  3 dÃ©cimales : valeurs trop uniques; risque sur-apprentissage ( mauvais voisinage si une maison juxtant un immeuble haute consommation des annÃ©es 60 vient Ã  Ãªtre dÃ©molie pour construire un beau batiment BBC !!
- CorrÃ©lation avec nombre d'Ã©tages => plotter sur une carte de chaleur downtown
"""

bestCorrelationsKeys={}
for i in x:
    print('_'*140+'\n')
    print('Best correlations for : '+i)
    print('_'*140+'\n')
    #x=correlations.loc[i].filter(regex='^(?!^(Site|Source).*|.*GFA$).*', axis=0).sort_values(ascending=False)
    y=correlations.loc[i].sort_values(ascending=False)
    print(y[:20])
    bestCorrelationsKeys[i]=list(y.index)

"""---
### ß· EnergyStarScore ?
- Best correlations : SiteEUIWN IntensitÃ© Usage EnergÃ©tique : 30%
- http://1.x24.fr/a/jupyter/seattle/scatRegressionHeatmap-energystarscore-siteeuiwn-kbtu-sf--0.3.webp
"""

i='ENERGYSTARScore'
y=correlations.loc[i].sort_values(ascending=False)
print(y[:20])

"""---
## IO :: export results checkpoint
---
"""

#'Electricity-KBtu/Gfa,SiteEnergyUseWN-KBtu/Gfa,Steam-KBtu/Gfa,GHGEmissions-Mt/Gfa,NaturalGas-KBtu/Gfa,OtherFuelUse-kBtu/Gfa,Electricity(kBtu),SiteEUI(kBtu/sf),SiteEUIWN(kBtu/sf),SiteEnergyUse(kBtu),SiteEnergyUseWN(kBtu),SourceEUI(kBtu/sf),SourceEUIWN(kBtu/sf),SteamUse(kBtu),GHGEmissions(MetricTonsCO2e),GHGEmissionsIntensity(kgCO2e/ft2),NaturalGas(kBtu)'.split(',')
#energies=x
#save(globals(),'df,results,df2015,df2016,x1,y1,x2,y2,row,heatmap'.split(','),'export1')#globals().update(resume('export1'))

"""---
### LargestPropertyUseType : Violins
- Afin de prÃ©dire les Ã©missions carbone d'un batiment GHGEmissionsIntensity,(kgCO2e/ft2), nous avons toutes ces variables Ã  disposition dans les meilleures corrÃ©lations dont l'usage de gaz est la premiÃ¨re, puis le type de RÃ©sidence/Commerce/FacilitÃ©/Usine prÃ©sente dans ce dernier d'oÃ¹ l'utilitÃ© des dummies sur ce dernier

- LargestPropertyUseType                   0.245083
- Energy/Power Station                     0.234361 ( le batiment est une centrale thermique => la surface industrielle va t-elle Ãªtre un facteur multipliant ? )
- Multifamily Housing                      0.231731
- Laboratory                               0.206927
- Other - Recreation                       0.199315
- Supermarket/Grocery Store                0.192647
- Hospital (General Medical & Surgical)    0.168149
- BuildingType                             0.167943
- YearBuilt                                0.149865
- Hotel                                    0.125551
- Fitness Center/Health Club/Gym           0.122078
- SecondLargestPropertyUseType             0.105966
- Swimming Pool                            0.104221
- Restaurant                               0.104108
- Other - Technology/Science               0.092024
---
"""

plt.rcParams["figure.figsize"] = (12,12)
i='GHGEmissionsIntensity(kgCO2e/ft2)';j='NaturalGas-KBtu/Gfa'
#scatter(train,i,j)
snsscat(train,i,j,opacity=0.5,color=[1,0,0.3,0.5],kind='reg',fn='ghg-gaz.png',minx=train[i].min(),maxx=train[i].max(),miny=train[j].min(),maxy=train[j].max())#Reg

cats='LargestPropertyUseType'.split(',')
#j='LargestPropertyUseType';
for cat in cats:
  print('}'+cat+'{')
  for i in energies:
    print('_'*180)
    print(i,end=' : ')
    #i='GHGEmissions(MetricTonsCO2e)';
    ts=20;
    f, ax = plt.subplots(figsize=(70,12))#large !! 70/12
    #ax.set(yscale="log")#, yscale="log"
    ax.set_xlim(auto=True)
    sns.set_style("ticks")
    ax.set_xticklabels(labels=train[i],rotation=(90),fontsize = 10,backgroundcolor='white')     

    #sns.regplot("x", "y", data, ax=ax, scatter_kws={"s": 100}), violin polot x axis not continuons
    fn='violinPerCatNoLog-'+filename(i+'-'+cat)+'.png';
    y=sns.violinplot( y=train[i], x=train[cat], ax=ax,scale='width',height=40,gridspec_kws={'wspace':.03})
    #!find . -name '*.png' -o -name '*.webp' -exec rm {} \;    
    y.get_figure().savefig(fn, bbox_inches='tight',dpi=dpi);
    webp(fn);#show()
    #y.get_figure()
#plt.savefig(fn, bbox_inches='tight'); ftpput(fn) 

#df[(df[j]=='Hospital (General Medical & Surgical)')][['PropertyName',i]]
#', '.join(map(str,train[j].unique()))

"""---
## Hue Pairplots ( classifiÃ©s )
- http://1.x24.fr/a/jupyter/seattle/violinPerCatNoLog-electricity-kbtu--largestpropertyusetype.webp #Datacenter
-> Tracer le plus grand pairplot jamais rÃ©alisÃ© avec un bon processeur une bonne fois pour toutes en exportant le .png rÃ©sultant afin de voir chacune des corrÃ©lations sous forme graphique ..
---
"""

df=train
plt.rcParams['figure.figsize']=(320,20);#Really Large png, cant be converted to webp with width > 16383 
#ValueError: max must be larger than min in range parameter.
# https://github.com/stanleychris2/graph-a-day/blob/master/2:7:16%20-%202014%20San%20Francisco%20Energy%20Score%20by%20Zip.ipynb?short_path=b066f80
if True:
  import matplotlib.image as mpimg
  #plt.rcParams['figure.figsize']=(320,320)#hugeeeeFIGSize for All parameters in 
  dfKeys=df.keys()

#print(df.dtypes['YearsENERGYSTARCertified'])
  for i in bestCorrelationsKeys.keys():
    if((df.dtypes[i]==object) | (i.find('z_')>-1)):
      print('skipping:'+i)
      continue;

    #print(i+' is '+str(df.dtypes[i]))#float64,object is okay for x axis ( categorized )      
    top30=bestCorrelationsKeys[i][:30];#respectivement
    for j in top30:
      if(j not in dfKeys):
        rm.append(j);
      #print(j)
      elif(j.find('z_')>-1):
        rm.append(j);
      elif('z_' in j):#splitted dummie data
        rm.append(j);
      elif(j=='ThirdLargestPropertyUseType'):
        rm.append(j);
      elif(df.dtypes[j]==object):
        rm.append(j);
    
    if(True & (i in top30)):#Removes itself Mutually Exclusive
      rm.append(i);#remove self key which has 100% correlation

    #print('.Removed plot data : '+','.join(rm))
    for z in rm:
      if(z in top30):
        top30.remove(z)
    #top30

    x='top30correlations-pairplot-scatter.'+filename(i)+'.png'
    #getFile(x);#fatal
    if(False & os.path.exists(x)):
      print('resuming:'+x)
      img=mpimg.imread(x);imgplot=plt.imshow(img);show()
    else:
    #g.fig.set_figheight(320);g.fig.set_figwidth(320);
  #AttributeError: 'PairGrid' object has no attribute 'get_figure'  
      
#Electricity(kBtu)/ThirdLargestPropertyUseType
#sns.pairplot ValueError: could not convert string to float: '2015, 2014' After : YearsEnergyStarCertified    ,
#uniqueValuesPerColumn('Electricity(kBtu)',df)df['Electricity(kBtu)'].unique(),NotFound:top30correlations-pairplot-scatter.electricitykbtu.png,ValueError: could not convert string to float: 'Restaurant'
#print(i)
#diag_kind='kde', <= seulement si stats croisÃ©es
      #sns.pairplot(df,kind="scatter",height=10,aspect=1,y_vars=[i],x_vars=top30).fig.savefig(x,bbox_inches='tight');#width=height*aspect
      sns.pairplot(df,kind="scatter",height=10,aspect=1,y_vars=[i],x_vars=top30,hue='LargestPropertyUseType').fig.savefig('hue'+x,bbox_inches='tight',dpi=dpi);#width=height*aspect
      webp('hue'+x);#show()

#Erreur rencontrÃ©es en chemin ...
if False:
  #ThirdLargestPropertyUseType
  i='ThirdLargestPropertyUseType';uniqueValuesPerColumn(i,df)#ValueError: could not convert string to float: 'Restaurant'
  i='GHGEmissions(MetricTonsCO2e)';df[i]=df[i].astype(float)
  df.dtypes[i]
  df[i]=df[i].astype(float);  

dpi=100
plt.rcParams['figure.figsize']=(20,6);#Restrict back dimensions

"""#.-------------------------------------------------------------

---
## ðŸ—²ðŸ—² TPU ðŸ—²ðŸ—²
- Bring it inline if available
---
"""

import tensorflow as tf
if 'select right processor type':#not tpu:
  try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
  except ValueError:
    tpu = None

  if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
    message('tpu inline')
  else:
    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
    message('not tpu')
    strategy = tf.distribute.get_strategy()     
     
clear_output()
p("REPLICAS: ", strategy.num_replicas_in_sync)
nbSplits=10;bs=8*128;roundIt=True
p(date())

"""---
# III) => Models
---
- L'inconvÃ©nient principal Ã  faire rentrer les donnÃ©es dans le modÃ¨le rÃ©side dans chacune des valeurs vides, ou infinies ou de type chaine de caractÃ¨res, la fonctione get_dummies convertit chacune des valeurs uniques d'un champ 'string' en autant de colonne boolÃ©enne donc il faut cloner le dataframe, puis lui retirer certaines colonnes non pertinentes afin d'en extraire ensuite les "dummies".
- On divise donc arbitrairement ce dataframe en 80% de donnÃ©es pour dÃ©duire le modÃ¨le et les 20% restants afin d'estimer sa prÃ©cision
---
"""

#globals().update(resume('export1'))

allModelsP={};
allModelsP['dummy']=sklearn.dummy.DummyRegressor
allModelsP['LinearSVR']=sklearn.svm.LinearSVR
allModelsP['OrthogonalMatchingPursuit']=sklearn.linear_model.OrthogonalMatchingPursuit
allModelsP['XGBRegressor']=xgboost.XGBRegressor
allModelsP['RandomForestRegressor']=sklearn.ensemble.RandomForestRegressor
allModelsP['ExtraTreesRegressor']=sklearn.ensemble.ExtraTreesRegressor
allModelsP['AdaBoostRegressor']=sklearn.ensemble.AdaBoostRegressor
allModelsP['GradientBoostingRegressor']=sklearn.ensemble.GradientBoostingRegressor
allModelsP['KernelRidge']=sklearn.kernel_ridge.KernelRidge
allModelsP['PassiveAggressiveRegressor']=sklearn.linear_model.PassiveAggressiveRegressor
allModelsP['MLPRegressor']=sklearn.neural_network.MLPRegressor
allModelsP['LinearRegression']=sklearn.linear_model.LinearRegression
allModelsP['Ridge']=sklearn.linear_model.Ridge
allModelsP['HistGradientBoostingRegressor']=sklearn.ensemble.HistGradientBoostingRegressor
allModelsP['KNeighborsRegressor']=sklearn.neighbors.KNeighborsRegressor
allModelsP['HuberRegressor']=sklearn.linear_model.HuberRegressor
allModelsP['Lars']=sklearn.linear_model.Lars
allModelsP['LassoLars']=sklearn.linear_model.LassoLars
allModelsP['BayesianRidge']=sklearn.linear_model.BayesianRidge
allModelsP['SVR']=sklearn.svm.SVR
allModelsP['StackingRegressor']=sklearn.ensemble.StackingRegressor
allModelsP['DecisionTreeRegressor']=sklearn.tree.DecisionTreeRegressor
#ValueError: Invalid parameter random_state for estimator SVR(
import inspect
hasRandomState=[]
for i in allModelsP.keys():
  has='random_state' in inspect.getargspec(allModelsP[i])[0]    
  #has='random_state' in dir(allModels[i])#SVR false positive
  if has:    
    hasRandomState+=[i];
p('Models having random state : '+' , '.join(hasRandomState))
#
allModels={};
allModels['PassiveAggressiveRegressor']=sklearn.linear_model.PassiveAggressiveRegressor()
allModels['dummy']=sklearn.dummy.DummyRegressor()
allModels['LinearSVR']=sklearn.svm.LinearSVR()
allModels['OrthogonalMatchingPursuit']=sklearn.linear_model.OrthogonalMatchingPursuit()
allModels['XGBRegressor']=xgboost.XGBRegressor()
allModels['GradientBoostingRegressor']=sklearn.ensemble.GradientBoostingRegressor()
allModels['KernelRidge']=sklearn.kernel_ridge.KernelRidge()
allModels['MLPRegressor']=sklearn.neural_network.MLPRegressor()
allModels['LinearRegression']=sklearn.linear_model.LinearRegression()
allModels['Ridge']=sklearn.linear_model.Ridge()
allModels['HistGradientBoostingRegressor']=sklearn.ensemble.HistGradientBoostingRegressor()
allModels['KNeighborsRegressor']=sklearn.neighbors.KNeighborsRegressor()
allModels['HuberRegressor']=sklearn.linear_model.HuberRegressor()
allModels['Lars']=sklearn.linear_model.Lars()
allModels['LassoLars']=sklearn.linear_model.LassoLars()
allModels['BayesianRidge']=sklearn.linear_model.BayesianRidge()
allModels['SVR']=sklearn.svm.SVR()
allModels['StackingRegressor']=sklearn.ensemble.StackingRegressor([('lr',sklearn.linear_model.RidgeCV()),('svr', sklearn.svm.LinearSVR(random_state=42))],sklearn.ensemble.RandomForestRegressor(random_state=42))
allModels['RandomForestRegressor']=sklearn.ensemble.RandomForestRegressor()
allModels['ExtraTreesRegressor']=sklearn.ensemble.ExtraTreesRegressor()
allModels['AdaBoostRegressor']=sklearn.ensemble.AdaBoostRegressor()
allModels['DecisionTreeRegressor']=sklearn.tree.DecisionTreeRegressor(random_state=42);

if False:
  param_grid={}
  randomKeys=range(1,nbRdKeys)
  npRandomKeys=range(1,nbRdKeys)

  my_cv=5;#sklearn.model_selection.KFold()

  nbRdKeys=60
  models=allModels
  for energy in toPredict:  
    print(energy)
    x1,y1,x2,y2,y_mean,mean=getSplitted(energy,dfNoEnergies)
    k1='test-rd-'+energy
    if k1 not in r2s.keys():
      r2s[k1]={};
    bestRandom[energy]={}
    for i in hasRandomState:
      if i in models.keys():
        gsm(allModels,i,k1,abort=True)    
        bestRandom[energy][i]=bsr=bestParameters[k2]['random_state']
        print(i+'\tr2:'+str(round(r2,2))+'\trd:'+str(bsr))
    print(arsort(bestRandomStateValues))
    break
  #del allModels;  
  assert(False)

  globals().update(resume('prepa1'))#avoir le split x1,x2
  #d'oÃ¹ l'importance de conserver plusieurs random_state : Nombreux sont ceux qui affectent 42, afin de fournir de meilleurs modÃ¨les sur ceux qui acceptent ce paramÃ¨tre !


  #explication des classements des valeurs
  cle='test-rd-Electricity(kBtu)-LinearSVR'
  scores={}
  resultats=list(cvResults[cle]['rank_test_score'])
  j=0;
  for i in resultats:
    k=cvResults[cle]['params'][j]
    scores[i]=k
    j+=1;
  print(sorted(scores.items()))



  print(grid_search.cv_results_.rank_test_score)
  grid_search_cv
  results = pd.DataFrame(grid.cv_results_.rank_test_score)
  results.sort_values(by='rank_test_score', inplace=True)


  allModels={};
  for i in allModelsP.keys():
    p={}
    if(i=='StackingRegressor'):
      p=[('lr',sklearn.linear_model.RidgeCV()),('svr', sklearn.svm.LinearSVR(random_state=42))],sklearn.ensemble.RandomForestRegressor(random_state=42)
    allModels[i]=allModelsP[i](p)
  #allModels['StackingRegressor']

"""---
##A) PrÃ©pa / Splitting Data# 
- Peut influencer sur les scores des modÃ¨les entrainÃ©s ci dessous
- NÃ©cessitÃ© : non shuffling splits afin de pouvoir comparer les modÃ¨les sur les mÃªmes jeux d'entrainements !!
---
"""

#resume();#print(acy.keys())#YearsENERGYSTARCertified
#globals().update(resume('export1'))
y_means={};resultss={};resultsArray={};splittedData={};mainPrediction={};models={};history={};cvResults={};
param_grid={};voting_grid={};ev={};fi={};pred={};bestPredictions={};t={};bestParameters={};r2s={}#reset

grid_search=0;_df=0;k2=0;
my_cv=nbFold=5;includeEnergyStar=0;includeOriginalDf=0;sshuffleData=0;stopAtOne=1;#more accurate predictions
#obj:73kilo, barracuda !
pd.set_option('display.max_columns',train.shape[1])

train['zip']=train['zip'].astype('int')
train['Outlier']=train['Outlier'].astype('str')
train['DefaultData']=train['DefaultData'].astype('bool')#works with No,False
train['LargestPropertyUseType']=train['LargestPropertyUseType'].astype('str')
#train['YearsENERGYSTARCertified']=train['YearsENERGYSTARCertified'].astype('str')
train['ThirdLargestPropertyUseType']=train['ThirdLargestPropertyUseType'].astype('str')
train['SecondLargestPropertyUseType']=train['SecondLargestPropertyUseType'].astype('str')
#train['TaxParcelIdentificationNumber']=train['TaxParcelIdentificationNumber'].astype('str')#invalide litterals for int .. dropped

#YearsENERGYSTARCertified,
#dropper les relevÃ©s que l'ont aura pas sur les nouveaux batiments .. 
if True:
  labelEncoded=train.copy(deep=True);#&

  #Original PrÃ©parÃ©
  if True:
    for col in labelEncoded:
      if(labelEncoded.dtypes[col]==object):
        print('converting:'+col)
        labelEncoded[col]=sklearn.preprocessing.LabelEncoder().fit_transform(labelEncoded[col].astype('str'))
      elif col.startswith('z_'):#dummies are boolen
        labelEncoded[col]=labelEncoded[col].astype('bool')
      else:
        labelEncoded[col]=labelEncoded[col].fillna(0).astype('float')

  dfNewKeys=labelEncoded[newKeys]
  labelEncoded=labelEncoded.drop(newKeys,axis=1);#Toutes sauf newKeys
  labEncCols=list(labelEncoded.columns.values)

  print(datetime.datetime.now())
  print(labelEncoded.dtypes)

  dfNoEnergies=labelEncoded.copy(deep=True);#& celui sur lequel on va travailler Ã©galement en retirant des valeurs
  df2Energies=dfNoEnergies[energies]#mise de cotÃ©s une fois converties en float avec fillna
  dfNoEnergies=dfNoEnergies.drop(energies,axis=1)#puis dropÃ©es
  others=list(dfNoEnergies.columns.values);#cols non energies
save(globals(),['allModels','allModelsP'],'prepa1')
#globals().update(resume('prepa1'))

"""---
## B) Models ð’€±
- ( on n'utilise qu'une seule mÃ©trique que l'on cherche Ã  prÃ©dire : Electricity Kbtu )
- Dummy sur un vote correspond Ã  tricher en induisant la valeur mÃ©diane ou moyenne des valeurs Ã  prÃ©dires dont nous ne sommes pas sensÃ©s disposer
---

---
#### 1) Comparaisons des modÃ¨les sur les donnÃ©es d'origine avec tous les champs fortement corrÃ©lÃ©es
----
- Sur le dataframe originel sans dropper les colonnes Ã©nergies afin de mesurer les Ã©carts sur les prÃ©dictions => on voit que ceci fonctionne trÃ¨s bien http://1.x24.fr/a/jupyter/seattle/scatterPredictionsNoParameters-electricity-kbtu-gfa-linearregression-1.0.webp
- Certains modÃ¨les performent mieux que d'autres, mÃªme en prÃ©sence de tous les relevÃ©s !!
- => ce qui permet de disqualifier les modÃ¨les prÃ©sentants de faibles performances vis Ã  vis de ce dataset
- <b><u>Attention cependant Ã  tester multiples random_state pour le retour du r2score pour certains modÃ¨les nÃ©cessitant plusieurs initialisations alÃ©atoires</u></b>
- On constate que la rÃ©gression linÃ©aire de base suffit le cas Ã©cheant !
---
"""

toPredict='Electricity(kBtu)'.split(',')
energy=toPredict[0]
rd=42
dpi=100

x1,y1,x2,y2,y_mean,mean=getSplitted(energy,labelEncoded.drop([energy],axis=1))
Kbtu_y_mean=ElectKbtu_y_mean=y_mean
Kbtu_mean=mean
Kbtu_y2=y2

import sklearn.tree
models={}
models['DecisionTreeRegressor']=sklearn.tree.DecisionTreeRegressor(random_state=rd);
models['dummy']=sklearn.dummy.DummyRegressor()
models['LinearRegression']=sklearn.linear_model.LinearRegression(n_jobs=-1)  
models['Lasso']=sklearn.linear_model.Lasso(random_state=rd)#Ridge less sensitive to outliers(abs)
models['ElasticNet']=sklearn.linear_model.ElasticNet(random_state=rd)#=Ridge + Lasso Penalties with their own Î»

models['ExtraTreesRegressor']=sklearn.ensemble.ExtraTreesRegressor(random_state=rd)
models['RandomForestRegressor']=sklearn.ensemble.RandomForestRegressor(random_state=rd)


models['HistGradientBoostingRegressor']=sklearn.ensemble.HistGradientBoostingRegressor(random_state=rd) 
models['GradientBoostingRegressor']=sklearn.ensemble.GradientBoostingRegressor(random_state=rd)  
models['DecisionTreeRegressor']=sklearn.tree.DecisionTreeRegressor(random_state=rd)
models['KNeighborsRegressor']=sklearn.neighbors.KNeighborsRegressor()
models['HuberRegressor']=sklearn.linear_model.HuberRegressor()
models['Lars']=sklearn.linear_model.Lars()
models['LassoLars']=sklearn.linear_model.LassoLars()
models['BayesianRidge']=sklearn.linear_model.BayesianRidge()  
models['LinearSVR']=sklearn.svm.LinearSVR(random_state=rd)
models['OrthogonalMatchingPursuit']=sklearn.linear_model.OrthogonalMatchingPursuit()  
models['SVR']=sklearn.svm.SVR()
models['StackingRegressor']=sklearn.ensemble.StackingRegressor([('Ridge',sklearn.linear_model.RidgeCV()),('BayesianRidge', sklearn.linear_model.BayesianRidge()  )],sklearn.ensemble.RandomForestRegressor(n_estimators=300))
models['dummy']=sklearn.dummy.DummyRegressor()
models['KernelRidge']=sklearn.kernel_ridge.KernelRidge()#0.72 !!!
models['PassiveAggressiveRegressor']=sklearn.linear_model.PassiveAggressiveRegressor(random_state=rd)
models['MLPRegressor']=sklearn.neural_network.MLPRegressor(random_state=rd)#
models['LinearRegression']=sklearn.linear_model.LinearRegression()
models['Ridge']=sklearn.linear_model.Ridge(random_state=rd)
models['XGBRegressor']=xgboost.XGBRegressor(random_state=rd)#~has importance metrics ;) -> when not voted
models['AdaBoostRegressor']=sklearn.ensemble.AdaBoostRegressor(random_state=rd)#trunks=Stumps
allModels1=models.copy()
timePerModel={}
scorePerModel={}
timePerModel[1]={}
scorePerModel[1]={}

for model in models:
  start = tim()
  models[model].fit(x1,x2)
  predictions=models[model].predict(y1)
  consommation=tim() - start
  timePerModel[1][model]=consommation
  
  predMean=predictions.mean()
  r2=sklearn.metrics.r2_score(y2,predictions) 
  r2s['Original-'+energy+model]=r2;

  scorePerModel[1][model]=r2
  print(model+' = time:'+str(consommation)+' , score :'+str(r2))
    
  _df=pd.DataFrame(list(zip(y2,predictions,y_mean)),columns=['Real','Guess','Mean'])
  fn='sp-npFullDF-'+filename(energy+'-'+model)+'-'+str(round(r2,2))+'.png'  

  f=_df[['Real','Guess','Mean']].plot(alpha=0.4,style=['o','r^','g-'],title='DonnÃ©es RÃ©elles vs prÃ©dictions '+energy+'-'+model+' - '+str(round(r2,2)));    
  #plt.figure(figsize=(24,6))
  plt.yscale('log');
  f.get_figure().savefig(fn,bbox_inches='tight',dpi=dpi);#results in nothing !!!!
#Maximum width and height allowed is 16383 pixels  
  #!stat sp-npFullDF-electricity-kbtu--decisiontreeregressor-0.11.png
  webp(fn);#show();  
  #assert(False)

display(arsort(timePerModel[1]))
display(arsort(scorePerModel[1]))


if False:
  os.system("find . -name '*.webp' -exec rm -f {} \;find . -name '*.png' -exec rm -f {} \;");
  def webp(x):
    if(GG('webpBroken')):
      show();
      return x
    x2 = x.replace('.png', '') + '.webp'
    #p(x,'=>',x2)
    res=webplib.cwebp(x, x2, '-q 70')
    if res['stderr']:
      err=res['stderr'].decode('ascii')
      if "cannot open input file" in err:
        p(err);
        assert(False)
      if "Cannot encode picture as WebP" in err:
        p(err)
        assert(False)
    if removepng:
      os.system('rm -f ' + x)
    if sendimages2ftp:
      ftpput(x2)
    if(GG('dontshow')):
      plt.close()
    return x2
  f.get_figure().savefig(fn,bbox_inches='tight',dpi=100);#results in nothing !!!!  
  webp(fn);#show();  
  #f.get_figure().savefig(fn,bbox_inches='tight');#results in nothing !!!!,dpi=30

"""---
#### 2) Amputation : Df sans les mesures Ã©nergÃ©tiques & GFA
---
- La suppression des colonnes rend la prÃ©vision plus difficile qu'auparavant ..
- Electricity Kbtu > Ridge : http://1.x24.fr/a/jupyter/seattle/spNoEnergies-electricity-kbtu--ridge01-0.25.webp
- http://1.x24.fr/a/jupyter/seattle/spNoEnergies-electricity-kbtu--bayesianridge-0.83.webp #basÃ© sur score r2 que nous ne somme pas sensÃ© avoir
---
 - Electricity-KBtu/Gfa > ExtraTrees : http://1.x24.fr/a/jupyter/seattle/spNoEnergies-electricity-kbtu-gfa-extratreesregressor-0.83.webp ( sans multiplication )
 - http://1.x24.fr/a/jupyter/seattle/spNoEnergiesMultiplied-electricity-kbtu-gfa-stackingregressor-0.95.webp #0.95
 ---
- => Electricity Kbtu/Gfa plus faÃ§ile Ã  prÃ© dire que Kbtu tout cours, du moins avec les modÃ¨les "simples", grÃ¢ce aux corrÃ©lations
- Puis reconstruction en le multipliant par la surface du batiment > on passe de  <a href='http://1.x24.fr/a/jupyter/seattle/spNoEnergies-electricity-kbtu-gfa-linearregression--21.12.webp'>-21.12</a> Ã  <a href='http://1.x24.fr/a/jupyter/seattle/spNoEnergiesMultiplied-electricity-kbtu-gfa-linearregression-0.93.webp'>0.93</a> ( meilleur modÃ¨le Ã  l'aveugle )
---
- Selon le feat engineering ou pas, les modÃ¨les nous retournent diffÃ©rents coefficients de dÃ©termination : Bayesian Ridge performe le mieux ( surtout une fois avoir multipliÃ© la valeur prÃ©dite par la surface totale du batiment )
---
"""

#save(globals(),'df,results,df2015,df2016,x1,y1,x2,y2,row,heatmap,df2Energies'.split(','),'amputee1')#globals().update(resume('amputee1'))
toPredict='Electricity-KBtu/Gfa,Electricity(kBtu)'.split(',')
timePerModel[2]={}
scorePerModel[2]={}

for energy in toPredict: 
  print('_'*180) 
  print(energy)
  x1,y1,x2,y2,y_mean,mean=getSplitted(energy,dfNoEnergies)
  for model in models:
    start=tim()
    models[model].fit(x1,x2)
    predictions=models[model].predict(y1)
    consommation=tim() - start
    timePerModel[2][energy+':'+model]=consommation    
    predMean=predictions.mean()
    r2=sklearn.metrics.r2_score(y2,predictions) 
    r2s['Empty-'+energy+':'+model]=r2;

    scorePerModel[2][energy+':'+model]=r2

    print(model+' : '+str(r2))
      
    _df2=pd.DataFrame(list(zip(y2,predictions,y_mean)),columns=['Real','Guess','Mean'])
    x='spNoEnergies-'+filename(energy+'-'+model)+'-'+str(round(r2,2))+'.png'
    f=_df2[['Real','Guess','Mean']].plot(alpha=0.4,style=['o','r^','g-'],title='DonnÃ©es RÃ©elles vs prÃ©dictions '+energy+'-'+model+' - '+str(round(r2,2)));  
    plt.yscale('log');f.get_figure().savefig(x,bbox_inches='tight',dpi=dpi);webp(x);#show();     

    if(energy=='Electricity-KBtu/Gfa'):
      multiplyPredicitions='PropertyGFATotal'#matchAgainst='Electricity(kBtu)'      
      predictions*=y1[multiplyPredicitions]
      r2=sklearn.metrics.r2_score(Kbtu_y2,predictions) 
      r2s['Empty-Multiplied-'+energy+model]=r2;
      scorePerModel[2][energy+':'+model+':multiplied']=r2
      print(model+' : multiplied : '+str(r2))

      _df2=pd.DataFrame(list(zip(Kbtu_y2,predictions,Kbtu_y_mean)),columns=['Real','Guess','Mean'])
      x='spNoEnergiesMultiplied-'+filename(energy+'-'+model)+'-'+str(round(r2,2))+'.png'
      f=_df2[['Real','Guess','Mean']].plot(alpha=0.4,style=['o','r^','g-'],title='DonnÃ©es RÃ©elles vs prÃ©dictions '+energy+'-'+model+' - '+str(round(r2,2)));  
      plt.yscale('log');f.get_figure().savefig(x,bbox_inches='tight',dpi=dpi);webp(x);#show();     

#display(arsort(timePerModel[2]))
display(arsort(scorePerModel[2]))

"""---
####3) +EnergyStar â‡©
- RÃ©-inclure cet indice au sein du dataframe ne permet pas de meilleurs rÃ©sultats sur les prÃ©visions, selon certaines mÃ©triques Ã  prÃ©dire â‡©
- 0.82 pour extratrees sur kbtu/gfa & 0.25 sur ridge sur kbtu de base => Cette variable n'a aucune incidence sur la qualitÃ© des prÃ©visions ( avec plus de modÃ¨les )
---
- <u><b>Extratrees sur kbtu/gfa : 0.83<b></u>
---
"""

#save(globals(),'df,results,df2015,df2016,x1,y1,x2,y2,row,heatmap,df2Energies'.split(','),'amputee2');#globals().update(resume('amputee2'))
dfEStar=dfNoEnergies.copy(deep=True);dfEStar['ENERGYSTARScore']=labelEncoded['ENERGYSTARScore'];#RÃ©-intÃ©grer cet indice !
toPredict='Electricity(kBtu)'.split(',')
timePerModel[3]={}
scorePerModel[3]={}
for energy in toPredict:
  x1,y1,x2,y2,y_mean,mean=getSplitted(energy,dfNoEnergies)

  for model in models:
    start=tim()
    models[model].fit(x1,x2)
    predictions=models[model].predict(y1)
    consommation=tim() - start
    timePerModel[3][energy+model]=consommation    
    predMean=predictions.mean()
    r2=sklearn.metrics.r2_score(y2,predictions) 
    r2s['EnergyStar-'+energy+model]=r2;
    scorePerModel[3][energy+model]=r2
    print(model+' : '+str(r2))
      
    _df=pd.DataFrame(list(zip(y2,predictions,y_mean)),columns=['Real','Guess','Mean'])
    x='spEnergyStarOnly-'+filename(energy+'-'+model)+'-'+str(round(r2,2))+'.png'
    f=_df[['Real','Guess','Mean']].plot(alpha=0.4,style=['o','r^','g-'],title='DonnÃ©es RÃ©elles vs prÃ©dictions '+energy+'-'+model+' - '+str(round(r2,2)));  
    plt.yscale('log');f.get_figure().savefig(x,bbox_inches='tight',dpi=dpi);webp(x);#show(); 

#display(asort(timePerModel[3]))
display(arsort(scorePerModel[3]))

"""---
##### Comparaison & Conclusion
- Bien que certaines variables, sous certains modÃ¨les soient moins prÃ©cis en retirant cette variable, 
- Dans la globalitÃ© on ne trouve aucune variable dont aucun modÃ¨le donne un meilleur rÃ©sultat en ajoutant la variable EnergyStarScore
- <u>**Global R2 Diff : 0**</u> => Les mÃªmes scores sont atteints avec ou sans ces indices
---
"""

#save(globals(),'df,results,df2015,df2016,x1,y1,x2,y2,row,heatmap,df2Energies'.split(','),'amputee3');#globals().update(resume('amputee3'))
p1='EnergyStar-'#EnergyStar-SiteEUIWN(kBtu/sf)RandomForestRegressor
p2='Empty-'#Empty-Electricity(kBtu)Ridge01

globalDiff=0
for energy in toPredict:  
  pf1=p1+energy;pf1k=[]#with EnergyStarScore
  pf2=p2+energy;pf2k=[]#without
  for key in r2s.keys():
    if key.startswith(pf1):
      pf1k+=[key]
    if key.startswith(pf2):
      pf2k+=[key]
#not done yet      
  if(len(pf1k)==0):
    continue;
  pf1kb = { nk: r2s[nk] for nk in pf1k }
  pf2kb = { nk: r2s[nk] for nk in pf2k }
  print(pf1kb)
  best1=arsort(pf1kb)[0]
  best2=arsort(pf2kb)[0]
  diff=best1[1]-best2[1];
  globalDiff+=diff
  if(diff>0):
    print(energy)
  print('Best score for '+energy+':with ESC:'+str(best1)+'/without:'+str(best2))
  print('\tDiff:'+str(diff))
  
print('Global R2 Diff : '+str(globalDiff/len(toPredict)))  

if False:
  print(r2s['Empty-SiteEnergyUseWN(kBtu)RandomForestRegressor'])
  #EnergyStar-Electricity(kBtu)Ridge01s
  differences={}
  for i1 in list(r2s.keys()):
    if(i1.startswith(p1)):
      i3=i1.replace(p1,'')
      i2=i1.replace(p1,p2)
      differences[i3]=diff=r2s[i1]-r2s[i2]
      #print('difference for '+i3+':'+str(diff))
  print(arsort(differences))#Cela pÃ©nalise la prÃ©diction des variables corrÃ©llÃ©es Ã  EnergyStarScore

"""---
####4) Ridge Individuels, puis votÃ©s par CV
---
- Grid Search ne semble pas renvoyer les meilleurs paramÃ¨tres, car il ne se base pas sur les rÃ©sultats du jeu de test !!
		 ==> Best Reel Model r2:[('Ridge20', 0.8308315043300891)] est diffÃ©rent du score retournÃ© par GridSearchCrossValidation
		 ==> Best Reel Cv Score:[('Ridge02', -48117657941128.47)] # somme rÃ©sidus
     ====> a un coeff r2 de -0.45 sur le jeu de test, mais -0.05 lorque votÃ©s parmis les 3 meilleurs : http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting.r2:-0.05.p:5.25.webp #cv
----
Sur le rÃ©el jeu de validation, plus alpha augmente plus le coefficient de dÃ©termination augmente ( attention nous mesurons ici ce coefficient basÃ© sur les rÃ©sultats que nous ne sommes pas sensÃ©s disposer )
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-ridge20.r2:+0.83.p:2.1.webp
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting.r2:+0.83.p:2.11.webp #r2
"""

#Qualification sur rd=42 sur jeu de test / non sensÃ© connaitre les rÃ©sultats finaux
y_means={};a=time();#scorer_
x1,y1,x2,y2,y_mean,mean=getSplitted('Electricity(kBtu)',dfNoEnergies)
param_grid={'Ridge01':{'random_state':[42]},'Ridge02':{'random_state':[42]},'Ridge1':{'random_state':[42]},'Ridge3':{'random_state':[42]},'Ridge5':{'random_state':[42]},'Ridge10':{'random_state':[42]},'Ridge20':{'random_state':[42]},'Ridge30':{'random_state':[42]}}
models={'Ridge01':sklearn.linear_model.Ridge(alpha=0.1),'Ridge02':sklearn.linear_model.Ridge(alpha=0.2),'Ridge1':sklearn.linear_model.Ridge(alpha=1),
'Ridge3':sklearn.linear_model.Ridge(alpha=3),
'Ridge5':sklearn.linear_model.Ridge(alpha=5),#best
'Ridge10':sklearn.linear_model.Ridge(alpha=10),
'Ridge20':sklearn.linear_model.Ridge(alpha=20),
'Ridge30':sklearn.linear_model.Ridge(alpha=30),
}

voted=2;votingOnReel=0;
modelsEvalutation();best=arsort(r2s[k1])[:1][0];print(best)
b='modelsNoParameters';t[b]=time()-a;print(t[b])

"""---
#####A) CrossValidation
---
- Retient les meilleurs paramÃ¨tres de prÃ©diction du jeu de test vis Ã  vis du jeu de validation
- Ne sont pas les mÃªme rÃ©sultats que prÃ©cÃ©demment : -0.45 sur le jeu de test rÃ©el avec meilleurs parametres :: {'alpha': 0.2, 'random_state': 42}
- 0.83 en mode voting - 0.76 avec rd:1, alpha:27
- On ne peut prÃ©dire les meilleurs paramÃ¨tres vis Ã  vis d'un jeu de donnÃ©es non connu, le plus de donnÃ©es on aura en entrÃ©e et le plus de chance d'Ãªtre exact sera vrai
=> Autant voter entre diffÃ©rentes valeurs de modÃ¨les
---
"""

a=time();#Hyper SpÃ©cialisation
x1,y1,x2,y2,y_mean,mean=getSplitted('Electricity(kBtu)',dfNoEnergies)
#autant de combinaisons que de permutations possibles !!!
#param_grid={'Ridge':{'alpha':[0,0.01,0.1,0.2,1,3,5,10,20,30,60,90],'random_state':[1,2,3,4,5,6,7,8,9,10,42]}}
param_grid={'Ridge':{'alpha':[0,0.01,0.1,0.2,1,3,5,10,20,23,25,26,27,28,29,30,35,60,90],'random_state':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,42]}}
models={'Ridge':sklearn.linear_model.Ridge()}
#voted single ..
votingOnReel=0;voted=2;modelsEvalutation(reset=1);best=arsort(r2s[k1])[:1][0];print(best)
b='modelsNoParameters';t[b]=time()-a;print(t[b])

"""---
###### B) BR CV
---
"""

a=time();#Hyper SpÃ©cialisation
#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html
param_grid={'BayesianRidge':{'n_iter':[1,5,10,100,300,1000],'tol':[1e-6,1e-3,1e-1,1,10,100,1000]}}#,'alpha_1':[1e-6],'alpha_2':[1e-6],'lambda_1':[1e-6],'lambda_2':[1e-6]
#for param in bestParameters[k2]:
models={'BayesianRidge':sklearn.linear_model.BayesianRidge()}
#voted single ..
#if((_ignoreParameters == 1) or (noPG)):
noPG=_ignoreParameters=0
list(dfNoEnergies.keys())
x1,y1,x2,y2,y_mean,mean=getSplitted('Electricity(kBtu)',dfNoEnergies)
votingOnReel=0;voted=2;modelsEvalutation(reset=1)

x1,y1,x2,y2,y_mean,mean=getSplitted('Electricity-KBtu/Gfa',dfNoEnergies)#as Globals
modelsEvalutation(reset=1,matchAgainst='Electricity(kBtu)');#'PropertyGFATotal'best=arsort(r2s[k1])[:1][0];print(best)

if False:#Totally out of Bounds !!
  modelsEvalutation(reset=1,multiplyPredicitions='PropertyGFATotal',matchAgainst='Electricity(kBtu)');
  b='br';t[b]=time()-a;print(t[b])

"""---
####5) Vote aveugle sans parametres
---
- Est une bonne chose ! http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting.r2:+0.6.p:3.25.webp
"""

nbRdKeys=2;noPG=1;votingOnReel=0;models={}
toPredict='Electricity(kBtu),Electricity-KBtu/Gfa,SiteEnergyUseWN-KBtu/Gfa,Steam-KBtu/Gfa,GHGEmissions-Mt/Gfa,NaturalGas-KBtu/Gfa,OtherFuelUse-kBtu/Gfa,ENERGYSTARScore,SiteEUI(kBtu/sf),SiteEUIWN(kBtu/sf),SourceEUI(kBtu/sf),SourceEUIWN(kBtu/sf),GHGEmissions(MetricTonsCO2e),GHGEmissionsIntensity(kgCO2e/ft2),SteamUse(kBtu),NaturalGas(kBtu),SiteEnergyUse(kBtu),SiteEnergyUseWN(kBtu)'.split(',')

models['LinearRegression']=sklearn.linear_model.LinearRegression()  
models['Lasso']=sklearn.linear_model.Lasso(random_state=42)#Ridge less sensitive to outliers(abs)
models['ElasticNet']=sklearn.linear_model.ElasticNet(random_state=42)#=Ridge + Lasso Penalties with their own Î»
models['ExtraTreesRegressor']=sklearn.ensemble.ExtraTreesRegressor(random_state=42)
models['RandomForestRegressor']=sklearn.ensemble.RandomForestRegressor(random_state=42)

a=time();voted=1;modelsEvalutation();
best=arsort(r2s[k1])[:1][0];print('_'*180);print(best)
b='votingWithoutParameterIs';t[b]=time()-a;print(t[b])
#('Ridge30', 0.8463011893002228) in 168.24609279632568 sec

"""---
####6) ModÃ¨les sans paramÃ¨tres estimÃ©s sur jeu de test rÃ©el
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-ridge20.r2:+0.83.p:2.1.webp #is cheating !
---
"""

nbRdKeys=2#minimal:range(1,2)=[1]
param_grid={};a=time();votingOnReel=1;
voted=0;modelsEvalutation();best=arsort(r2s[k1])[:1][0];print(best)
b='modelsNoParameters';t[b]=time()-a;print(t[b])

"""####7) ModÃ¨les avec paramÃ¨tres sur jeu test rÃ©el
---
    ==> Best Reel Model r2:[('Ridge20', 0.8308315043300891)] Ã©videmment, is cheating !
    ==> Best Reel Cv Score:[('ExtraTreesRegressor', -7307477498056.606)] sera apparement le meilleur modÃ¨le pour prÃ©dictions => 0.21 sur jeu rÃ©el
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting.r2:+0.6.p:3.25.webp > supÃ©rieur sur jeu rÃ©el
"""

toPredict='Electricity(kBtu)'.split(',')
votingOnReel=1;#noPG=1#no Parameters Grid
#pred={};r2s={};bestPredictions={}#reset
param_grid={
'dummy':{'strategy':['mean','median']},
'LinearRegression':{'normalize':[True,False]},
'Ridge':{'alpha':[0.01,10],'random_state':[42]},#lambda
'Lasso':{'alpha':[0.01,10]},#random_state,max_iter
'ElasticNet':{'alpha':[0.0001,0.01,10]},#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV
'RandomForestRegressor':{'n_estimators':[10,100],'random_state':[42]},
'ExtraTreesRegressor':{'n_estimators':[10,20,30,100,200],'random_state':[42]},
}
#ElasticNet', 0.84426018686573)
a=time();noPG=0;
voted=2;modelsEvalutation();best=arsort(r2s[k1])[:1][0];
print(bestParameters)
print('_'*180);print(best)
b='modelsOnlyWithParameters';t[b]=time()-a;
print(t[b])
votingOnReel=0;

"""---
##### CV Sans Grille sans rÃ©el
- plus rapide ! 
---
"""

voted=0;noPG=1;b='modelsOnlyNoParamNotReel';
a=time();modelsEvalutation();best=arsort(r2s[k1])[:1][0];
print(bestParameters)
print('_'*180);print(best)
t[b]=time()-a;

"""###### Best Scores"""

arsort(bss)#best CV scoring per model on blind crosscv validation :)

"""#### 8) GridSearchCV + params + voted only
- Excluant jeu test rÃ©el de validation
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting.r2:+0.6.p:3.25.webp
"""

votingOnReel=0;
toPredict='Electricity-KBtu/Gfa,Electricity(kBtu)'.split(',')
plt.rcParams["figure.figsize"] = (24,12)
nbRdKeysPerModel['RandomForestRegressor']=nbRdKeysPerModel['ExtraTreesRegressor']=2;#Prend plus de temps mais au moins ..
nbRdKeys=2;#limiter le nombre de clÃ©s alÃ©atoires en prÃ©sence d'autres paramÃ¨tres ==> Mais pas bon pour extratreeregressor

a=time();b='singleThenVotedWithGrid';voted=1;#voted=2;models then voting
arretAnticipe=0;stopAtOne=1;modelsEvalutation();#- on peut donc passer le paramÃ¨tre voting Ã  1 pour seulement opÃ©rer avec ce dernier ( mais on perd les paramÃ¨tres gridcv ou il faudrait les rÃ©-Ã©crire .. )
best=arsort(r2s[k1])[:1][0];bestPredictions[k1+'-'+best[0]]=pred[k1+'-'+best[0]];print('_'*180);print(best)#0.72 without voting_grid parameters
t[b]=time()-a;print(t[b])

"""######A) Faster without parameters
---
- Voting, puis multiplication par PropertyGFATotal : <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu-gfa-trimmed-voting-m:propertygfatotal-a:electricity-kbtu--ridge01-ridge02-ridge1-ridge3-ridge5-ridge10-ridge20-ridge30-linearregression-lasso-elasticnet-extratreesregressor-randomforestregressor-np.r2:+0.95.p:1.17.webp'> => 0.95 ( Meilleur Ã  l'aveugle )</a>
- <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu-gfa-trimmed-voting-m:propertygfatotal-a:electricity-kbtu--dummy-ridge01-ridge02-ridge1-ridge3-ridge5-ridge10-ridge20-ridge30-linearregression-lasso-elasticnet-extratreesregressor-randomforestregressor-np.r2:+0.96.p:1.0.webp'>+1 point avec dummy que nous ne sommes pas sensÃ©s utiliser</a>
"""

toPredict='Electricity-KBtu/Gfa'.split(',');voted=1;votingOnReel=0;noPG=1;
modelsEvalutation(multiplyPredicitions='PropertyGFATotal',matchAgainst='Electricity(kBtu)');#- on peut donc passer le paramÃ¨tre voting Ã  1 pour seulement opÃ©rer avec ce dernier ( mais on perd les paramÃ¨tres gridcv ou il faudrait les rÃ©-Ã©crire .. )

"""##### B) avec Parameters
- Cela est long et fournit un point de plus => <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu-gfa-trimmed-voting-m:propertygfatotal-a:electricity-kbtu--dummy-ridge01-ridge02-ridge1-ridge3-ridge5-ridge10-ridge20-ridge30-linearregression-lasso-elasticnet-extratreesregressor-randomforestregressor.r2:+0.97.p:0.95.webp'>0.97</a>
"""

toPredict='Electricity-KBtu/Gfa'.split(',');voted=1;votingOnReel=0;
#multiplie le nombre de paramÃ¨tres par candidat de maniÃ¨re exponentielle en fittant des paramÃ¨tres non appropriÃ©s pour chaque ..
modelsEvalutation(multiplyPredicitions='PropertyGFATotal',matchAgainst='Electricity(kBtu)');#- on peut donc passer le paramÃ¨tre voting Ã  1 pour seulement opÃ©rer avec ce dernier ( mais on perd les paramÃ¨tres gridcv ou il faudrait les rÃ©-Ã©crire .. )

"""---
####9) => Voted vs Models basÃ© sur score gridCV et non r2 !
- http://1.x24.fr/a/jupyter/seattle/GridCV-BestvsVoted-electricity-kbtu-.webp #voted proche de extratrees individuel
- http://1.x24.fr/a/jupyter/seattle/GridCV-4BestModels-electricity-kbtu-.webp
---
"""

k1='Electricity(kBtu)-trimmed'
r2sSorted=arsort(bss)
ok=[];

for i in range(0,2):
  if(k1+'-'+r2sSorted[i][0] not in pred.keys()):
    print('missing:'+r2sSorted[i][0]+'=> deleting')
    del r2sSorted[i]
  else:
    ok+=[r2sSorted[i]]

p1=pred[k1+'-'+r2sSorted[0][0]]
p2=pred[k1+'-'+r2sSorted[1][0]]
p3=pred[k1+'-'+r2sSorted[2][0]]
p4=pred[k1+'-'+r2sSorted[3][0]]
pv=pred[voted]
voted=lastVotingKey;

ziped=zip(ElectKbtu_y_mean,y2,p1,p2,p3,p4,pv)
_df=pd.DataFrame(list(ziped),columns=['Mean','Real',r2sSorted[0][0],r2sSorted[1][0],r2sSorted[2][0],r2sSorted[3][0],'voted'])

fn='GridCV-BestvsVoted-'+filename(energy)+'.png'
f=_df[['Mean','Real',r2sSorted[0][0],'voted']].plot(alpha=0.4,style=['k-','bo','r^','g*','m>','cx'],title='DonnÃ©es RÃ©elles vs prÃ©dictions '+energy);plt.yscale('log');
f.get_figure().savefig(fn);webp(fn);#show();

fn='GridCV-4BestModels-'+filename(energy)+'.png'
f=_df[['Mean','Real',r2sSorted[0][0],r2sSorted[1][0],r2sSorted[2][0],r2sSorted[3][0]]].plot(alpha=0.4,style=['k-','bo','r^','g*','m>','cx'],title='DonnÃ©es RÃ©elles vs prÃ©dictions '+energy);  
plt.yscale('log');f.get_figure().savefig(fn);webp(fn);#show();

"""# # -#-------------------------

---
###A) Features Importance
---
- In a decision tree avec evaluation GridCV des paramÃ¨tres
- http://1.x24.fr/a/jupyter/seattle/#q=FeatImportance
- <a href='http://1.x24.fr/a/jupyter/seattle/FeatImportance-electricity-kbtu--trimmed-extratreesregressor0.21.webp'>Extratrees : on voit trÃ¨s clairement PropertyGFATotal, nbfloors comme premier critÃ¨re ( retenu par scoring gridsearchcv )</a>
"""

toPredict='Electricity(kBtu)'.split(',')
param_grid={
'XGBRegressor':{'reg':['squarederror'],'objective':['reg:squarederror'],'learning_rate':[0.01,0.1,0.3,1],'random_state':[42]},
'RandomForestRegressor':{'n_estimators':[10,100,300,600],'random_state':[42]},
'ExtraTreesRegressor':{'n_estimators':[10,100,300,600],'random_state':[42]},
'AdaBoostRegressor':{'n_estimators':[10,600],'learning_rate':[0.01,10],'loss':['linear','square'],'random_state':[42]},    
'GradientBoostingRegressor':{'learning_rate':[0.001,0.01,0.1,1,2],'random_state':[42]},#like adaboost
}
nbRdKeys=2;#cela va prendre bcp de temps : rÃ©duire le nb de clÃ©s alÃ©atoires
models={}
models['XGBRegressor']=xgboost.XGBRegressor()#~has importance metrics ;) -> when not voted
models['RandomForestRegressor']=sklearn.ensemble.RandomForestRegressor()#~has importance metrics ;) -> when not voted
models['ExtraTreesRegressor']=sklearn.ensemble.ExtraTreesRegressor()
models['AdaBoostRegressor']=sklearn.ensemble.AdaBoostRegressor()#trunks=Stumps
models['GradientBoostingRegressor']=sklearn.ensemble.GradientBoostingRegressor()  

noPG=0;voted=0;modelsEvalutation();
best=arsort(r2s[k1])[:1][0];print(best)

"""###B) Mesures de Performances"""

print(arsort(t))

"""####1) Hyper-paramÃ¨tres ++
---
- Learning_rate ou alpha ou n_estimators pour les trees
- Best model r2 with voting is : voting, 0.65, moins qu'un modÃ¨le choisi individuellement, mais quelles sont les composantes principales ???
- 'BayesianRidge', 0.84 - valeurs restant dans la "norme" >> Il suffit parfois d'une unique valeur pour exploser les carrÃ©s rÃ©siduels
- Attention, parfois les paramÃ¨tres par dÃ©faut retournent de meilleurs valeurs
"""

nbCols=dfNoEnergies.shape[1]
bigParamGrid= {  
  'SVR':{'kernel':['rbf'],'degree':[3],'gamma':['scale'],'coef0':[0.0],'tol':[0.001],'C':[1.0],'epsilon':[0.1],'shrinking':[True],'cache_size':[200],'verbose':[False],'max_iter':[-1]},#rbf,'poly=> too much time','linear','sigmoid'
  'LinearSVR':{'random_state':[=0,42],'tol':[0.0001],'C':[1,100,400],'epsilon':[0,1,10],'loss':['epsilon_insensitive'],'fit_intercept':[True],'intercept_scaling':[1],'dual':[True],'verbose':[0],'random_state':[None],'max_iter':[1000]},#'C': 100, 'dual': True, 'epsilon': 1
  'OrthogonalMatchingPursuit':{'n_nonzero_coefs':[None,round(nbCols*0.1),round(nbCols*0.2),round(nbCols*0.9)],'precompute':[True,False]},#
  'Lars':{'n_nonzero_coefs':[1,500,np.inf]},
  'KernelRidge':{'alpha':[0.0001,0.01,1]},#,10 :: too much time !!!
  'LassoLars':{'alpha':[0.0001,0.01,1]},#,10
  'PassiveAggressiveRegressor':{'max_iter':[10,100,300,500]},
  'HuberRegressor':{'alpha':[0.0001,0.01,1,10],'max_iter':[10,100,300,500],'epsilon':[1.35],'tol':[0.00001]},
  'HistGradientBoostingRegressor':{'learning_rate':[0.0001,0.01,1,2]},#(loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)
  'StackingRegressor':{'cv':[30,60]},#estimators,final_estimator,cv:1,5,30
  
'ElasticNet':{'alpha':[0.0001,0.01,1,10]},#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV  
'ExtraTreesRegressor':{'n_estimators':[10,100,300]},
'RandomForestRegressor':{'n_estimators':[10,100,300]},

'AdaBoostRegressor':{'n_estimators':[10,600],'learning_rate':[0.01,10],'loss':['linear','square']},    
'BaggingRegressor':{'n_estimators':[100],'max_samples':[20],'max_features':[200]},#base_estimator:DecisionTree
'MLPRegressor':{'activation':['relu'],'hidden_layer_sizes':[(100,)],'solver':['adam'],'learning_rate':['constant','adaptive'],'learning_rate_init':[0.1,0.001]},#constant
'dummy':{'strategy':['mean','median']},#quantile,constant
'KNeighborsRegressor':{'n_neighbors':[1,10,21]},#list(range(1, 21))},
'XGBRegressor':{'reg':['squarederror'],'learning_rate':[0.01,0.1,0.3,1],'objective':['reg:squarederror']} ,#0.10, 0.15, 0.20, 0.25, },
'GaussianNB':{'var_smoothing':[0.1,0.000001]},#classifier
'GradientBoostingRegressor':{'learning_rate':[0.001,0.01,0.1,1,2]},#like adaboost
#ValueError: Invalid parameter verbose for estimator AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',n_estimators=50, random_state=None). Check the list of available parameters with `estimator.get_params().keys()`.
'Ridge':{'alpha':[0.1,1,10,30]},#lambda,_ridge UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.

'Lasso':{'alpha': np.logspace(-4, -0.5, 30)},#random_state,max_iter
'BayesianRidge':{'lambda_1':[0.001,0.000001]},
'LinearRegression':{'normalize':[True,False]},
'SGDRegressor':{'max_iter':[50000],'verbose':[1],'loss':['huber'],'learning_rate':['optimal'],'eta0':[0.1],'power_t':[0.35],'alpha':[0.2]},#learning rate
#(loss='squared_loss', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)  
#(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,max_depth=3, min_child_weight=1, missing=None, n_estimators=100,n_jobs=1, nthread=None, objective='reg:linear', random_state=0,reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,silent=True, subsample=1)  
'XGBRegressor2':{
#XGBRegressor ValueError: Parameter values for parameter (objective) reg:squarederror need to be a sequence(but not a string) or np.ndarray.      
#'objective':'reg:squarederror','verbose':[100] 
'learning_rate':    [0.05, 0.30] ,#0.10, 0.15, 0.20, 0.25, 
'max_depth'        : [3, 15],#4, 5, 6, 8, 10, 12, 
'min_child_weight' : [1, 7],#3, 5, 
#ValueError: Invalid parameter colsample_bytree for estimator GaussianNB(priors=None, var_smoothing=1e-09). Check the list of available parameters with `estimator.get_params().keys()`. 
'gamma'            : [0.0,  0.4],#0.1, 0.2 , 0.3,
'colsample_bytree' : [0.3,  0.7],#0.4, 0.5 ,
}
}

param_grid=bigParamGrid

"""####2) Ajout autres modÃ¨les
---
- Meilleurs modÃ¨les de prÃ©diction de la consommation Ã©lectrique directs : d'aprÃ¨s retour rÂ²
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-passiveaggressiveregressor-np-np.r2:+0.94.p:1.26.webp #<<<depends on random_states !!!
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-ridge20.r2:+0.83.p:2.1.webp
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-mlpregressor.r2:+0.83.p:2.09.webp
- http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting.r2:+0.81.p:2.23.webp
---
- A l'aveugle : <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-extratreesregressor.r2:+0.2.p:4.56.webp'>Voting 0.2</a>
"""

toPredict='Electricity(kBtu)'.split(',')
param_grid['LinearSVR']={'random_state':[0,42],'tol':[0.0001],'C':[1,100,400],'epsilon':[0,1,10],'loss':['epsilon_insensitive'],'fit_intercept':[True],'intercept_scaling':[1],'dual':[True],'verbose':[0],'random_state':[None],'max_iter':[1000]}
models=allModels1.copy()
ignoreParameters=1;voted=2;votingOnReel=0;noPG=0;#d'abord Ã©valuÃ©s individuellement, puis votÃ©s !
modelsEvalutation();
print('_'*180)
print('Best RÂ² score')
best=arsort(r2s[k1])[:1][0];print(best)
print('Best CV score')
best=arsort(bss)[:1][0];print(best)
#print(bestNP)

print('Best RÂ² score')
best=arsort(r2s[k1])[:1][0];print(best)
print('Best CV score')
best=arsort(bss)[:1][0];print(best)

# Commented out IPython magic to ensure Python compatibility.
# %%script false
# if False:#Timing ou performances trÃ¨s mauvaises
#   models['GaussianProcessRegressor']=sklearn.gaussian_process.GaussianProcessRegressor()
#   models['SGDRegressor']=sklearn.linear_model.SGDRegressor()
#   models['BaggingRegressor1-xtraTrees']=sklearn.ensemble.BaggingRegressor(sklearn.ensemble.ExtraTreesRegressor())#,max_samples=0.5, max_features=0.5
#   models['BaggingRegressor2-randomForest']=sklearn.ensemble.BaggingRegressor(sklearn.ensemble.RandomForestRegressor())
# #Classifiers
#   voted=2;modelsEvalutation();best=arsort(r2s[k1])[:1][0];print(best)
#   models['SVC']=SVC()#is linear or rbf 183 : une succession de zÃ©ros Ã©galement ..    
#   models['GaussianNB']=GaussianNB()#Naive Bayes : intersection de probabilitÃ©s ( King vs Face )'GaussianNB': array([0, 0, 0, ..., 0, 0, 0]),  
#   models['ExtraTreesClassifier']=ExtraTreesClassifier()
#   models['BaggingClassifier']=BaggingClassifier()
#   models['KNeighborsClassifier']=KNeighborsClassifier()
#   models['RandomForestClassifier']=RandomForestClassifier()
#   models['DecisionTreeClassifier']=DecisionTreeClassifier()
#   #100 => not enough Ram ? Huh .. :(
#   #models['RandomForestClassifier100']=RandomForestClassifier(n_estimators=100);      
#   models['AdaBoostClassifier']=AdaBoostClassifier()  
#   models['Perceptron']=Perceptron()    
# #RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier
# #models['LinearSVC']=LinearSVC()#is linear separator SVM
# #MLPClassifier()
#   models['SGDClassifier']=SGDClassifier()#squared loss
#   models['GradientBoostingClassifier']=GradientBoostingClassifier()  
#   models['LogisticRegression']=sklearn.linear_model.LogisticRegression(multi_class='auto',solver='saga',penalty='l1',C=5,max_iter=1000)#S shaped > Classifies Obese or Not
# #save(globals(),[],'export2');globals().update(resume('export2'))

"""##âˆ‘) Toutes Ã©nergies, en votation Ã  l'aveugle
---
<b><u>No Silver Bullet</u></b>, la multiplication par GFA reste la meilleure piste, sans validation des paramÃ¨tres GridSearch : pauvres performances globales
- http://1.x24.fr/a/jupyter/seattle/sp-energystarscore-trimmed-voting-linearregression-la.r2:-1.59.p:1.18.webp
- http://1.x24.fr/a/jupyter/seattle/sp-sourceeui-kbtu-sf--trimmed-voting-linearregression.r2:-12.38.p:3.37.webp
- http://1.x24.fr/a/jupyter/seattle/sp-siteeui-kbtu-sf--trimmed-voting-linearregression-l.r2:-1688973520.68.p:39363.32.webp
- http://1.x24.fr/a/jupyter/seattle/sp-siteeuiwn-kbtu-sf--trimmed-voting-linearregression.r2:-36.67.p:5.68.webp
- http://1.x24.fr/a/jupyter/seattle/sp-sourceeui-kbtu-sf--trimmed-voting-linearregression.r2:-12.38.p:3.37.webp
- http://1.x24.fr/a/jupyter/seattle/sp-sourceeuiwn-kbtu-sf--trimmed-voting-linearregressi.r2:-6.46.p:2.44.webp
- http://1.x24.fr/a/jupyter/seattle/sp-ghgemissions-metrictonsco2e--trimmed-voting-linear.r2:-38.77.p:21.7.webp
- http://1.x24.fr/a/jupyter/seattle/sp-ghgemissionsintensity-kgco2e-ft2--trimmed-voting-l.r2:-497.45.p:36.17.webp
- http://1.x24.fr/a/jupyter/seattle/sp-steamuse-kbtu--trimmed-voting-linearregression-las.r2:-43.38.p:83.95.webp
- http://1.x24.fr/a/jupyter/seattle/sp-naturalgas-kbtu--trimmed-voting-linearregression-l.r2:-44.48.p:13.78.webp
- http://1.x24.fr/a/jupyter/seattle/sp-siteenergyuse-kbtu--trimmed-voting-linearregressio.r2:-1.16.p:7.03.webp
- http://1.x24.fr/a/jupyter/seattle/sp-siteenergyusewn-kbtu--trimmed-voting-linearregress.r2:-20.49.p:6.42.webp
---
"""

#on Ã©vince les modÃ¨les prennant trop de temps ou avec de mauvais rÃ©sultats : SVR,StackingRegressor,KernelRidge
param_grid['AdaBoostRegressor']={'learning_rate':[0.01],'loss':['linear'],'n_estimators':[600]}
goodPerformanceWithParams='AdaBoostRegressor,lars,XGBRegressor'.split(',')
betterPerformanceWithoutParams='GradientBoostingRegressor,BayesianRidge,AdaBoostRegressor,ExtraTreesRegressor,RandomForestRegressor,OrthogonalMatchingPursuit,HistGradientBoostingRegressor,dummy,KNeighborsRegressor,HuberRegressor,LassoLars,LinearSVR,SVR,StackingRegressor,KernelRidge,PassiveAggressiveRegressor,MLPRegressor,LinearRegression,Ridge'.split(',')
for i in betterPerformanceWithoutParams:
  param_grid[i]={}

toPredict=toPredictBase.copy()
toPredict.remove('Electricity(kBtu)')#dÃ©jÃ  fait
models=allModels1.copy()
votingOnReel=0;shuffleData=0;stopAtOne=0;includeOriginalDf=0;includeEnergyStar=0;ignoreParameters=0;
#voted=1;noPG=0;TrÃ¨s mauvaise idÃ©e => Voting 7200 fits
voted=1;noPG=1#Oui ! Nous ne sommes pas sensÃ© avoir les rÃ©sultats r2
#voted=2;noPG=0;#straight Ahead, no parameters :: Mauvaise IdÃ©e: 

modelsEvalutation()
#models['SVR']=sklearn.svm.SVR()
#models['StackingRegressor']=sklearn.ensemble.StackingRegressor([('lr',sklearn.linear_model.RidgeCV()),('svr', sklearn.svm.LinearSVR(random_state=42))],sklearn.ensemble.RandomForestRegressor(n_estimators=10,random_state=42))
save(globals(),[],'checkpoint3')

"""###1) Vote avec retours sur performance des meilleurs modÃ¨les via CrossValidation
---
- La cross-validation Permet de meilleurs scores gÃ©nÃ©raux !
---
### UnanimitÃ© r2 & CV 
- http://1.x24.fr/a/jupyter/seattle/sp-energystarscore-trimmed-extratreesregressor.r2:+0.69.p:0.41.webp
- http://1.x24.fr/a/jupyter/seattle/sp-siteeui-kbtu-sf--trimmed-extratreesregressor.r2:+0.83.p:0.39.webp
- http://1.x24.fr/a/jupyter/seattle/sp-siteeuiwn-kbtu-sf--trimmed-extratreesregressor.r2:+0.83.p:0.37.webp
- http://1.x24.fr/a/jupyter/seattle/sp-sourceeui-kbtu-sf--trimmed-extratreesregressor.r2:+0.83.p:0.38.webpp
- http://1.x24.fr/a/jupyter/seattle/sp-sourceeuiwn-kbtu-sf--trimmed-extratreesregressor.r2:+0.83.p:0.36.webp

---
#### Divergences de choix 
- http://1.x24.fr/a/jupyter/seattle/sp-ghgemissions-metrictonsco2e--trimmed-lassolars.r2:+0.71.p:1.86.webp#r2
- http://1.x24.fr/a/jupyter/seattle/sp-ghgemissions-metrictonsco2e--trimmed-randomforestregressor.r2:+0.55.p:2.31.webp#cv
---
- http://1.x24.fr/a/jupyter/seattle/sp-ghgemissionsintensity-kgco2e-ft2--trimmed-extratreesregressor.r2:+0.73.p:0.93.webp#r2
- http://1.x24.fr/a/jupyter/seattle/sp-ghgemissionsintensity-kgco2e-ft2--trimmed-randomforestregressor.r2:+0.67.p:0.93.webp#cv
---
- http://1.x24.fr/a/jupyter/seattle/sp-steamuse-kbtu--trimmed-orthogonalmatchingpursuit.r2:+0.6.p:7.96.webp#r2
- http://1.x24.fr/a/jupyter/seattle/sp-steamuse-kbtu--trimmed-extratreesregressor.r2:+0.31.p:10.48.webp#cv
---
- http://1.x24.fr/a/jupyter/seattle/sp-naturalgas-kbtu--trimmed-extratreesregressor.r2:+0.57.p:1.34.webp#r2
- http://1.x24.fr/a/jupyter/seattle/sp-naturalgas-kbtu--trimmed-xgbregressor.r2:+0.38.p:1.61.webp#cv
"""

voted=2;noPG=0;votingOnReel=0;modelsEvalutation()
save(globals(),[],'checkpoint4')

"""###2) Voting sur R2 rÃ©el
---
- <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting-ridge01-ridge02-ridge1-ridge3-ridge5-ridge10-ridge20-ridge30-linearregression-lasso-elasticnet-extratreesregressor-randomforestregressor-np.r2:+0.61.p:3.2.webp'>Vote aveugle sans paramÃ¨tres : 0.61</a>
- Is Cheating : <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu--trimmed-voting-BayesianRidge,LinearSVR,MLPRegressor,OrthogonalMatchingPursuit.r2:+0.81.p:2.22.webp'>0.81 Via RÂ²</a>
- <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu-gfa-trimmed-voting-m:propertygfatotal-a:electricity-kbtu--dummy-ridge01-ridge02-ridge1-ridge3-ridge5-ridge10-ridge20-ridge30-linearregression-lasso-elasticnet-extratreesregressor-randomforestregressor.r2:+0.97.p:0.95.webp'> CrossValidation puis vote puis multiplication : 0.97</a>
"""

voted=2;noPG=0;votingOnReel=1;modelsEvalutation()
save(globals(),[],'checkpoint5')

"""###3) Foreach"""

for k in r2s.keys():
  print(k)
  print(arsort(r2s[k]))
  print('_'*180)

message('processes finished, data collected and archived')
print(cvResults.keys())
for i in cvResults.keys():
  print('_'*180)
  print(i)
  print(arsort(cvResults[i].rank_test_score))
assert(False)

"""#.-------------------------------------------------------------

---
#Conclusions
---
- On ne doit pas connaitre ni le r2 score ni utiliser le dummy regressor, voter directement entre plusieurs modÃ¨les semble avoir le meilleur effet sur les prÃ©visions, exclure tous les modÃ¨les ayant eu des performances / scores infÃ©rieurs aux prÃ©visions statistiques de ce dernier
- <u>**Voter entre plusieurs modÃ¨les peu rÃ©sulter en un meilleur coefficient de dÃ©termination, ou non **</u>
- Le meilleur modÃ¨le de prÃ©diction de la consommation Ã©lectrique est : le modÃ¨le votÃ©, sur la Cross Validation de tous les modÃ¨les, en retenant les 4 meilleurs, puis en multipliant le rÃ©sultat par la surface totale de l'immeuble : <a href='http://1.x24.fr/a/jupyter/seattle/sp-electricity-kbtu-gfa-trimmed-voting-m:propertygfatotal-a:electricity-kbtu--dummy-ridge01-ridge02-ridge1-ridge3-ridge5-ridge10-ridge20-ridge30-linearregression-lasso-elasticnet-extratreesregressor-randomforestregressor.r2:+0.97.p:0.95.webp'>Electricity Kbtu : 0.97</a>
---

Cross Validation sur du Dataframe en 20 Folds
"""

if False:#repeat prÃ©pa


  assert(False)



  from sklearn.tree import DecisionTreeRegressor
  cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)
  extr=clf = sklearn.ensemble.ExtraTreesRegressor()

  #test2 = df.loc[~df.index.isin(train2.index)]

  nFeatures=[1,2,10,20]#list(range(1,20))
  #('select', SelectKBest()),'select__k':nFeatures,
  pipe = Pipeline([('extr', extr)])#array of Tuple models
  param_grid={'extr__n_estimators':[100,300],'extr__random_state':[42],'extr__min_samples_leaf':[1,8]}#{'extr__n_estimators': 100, 'select__k': 1}

  scores = cross_val_score(clf, X, y, cv=cv, scoring='r2')

  search=GridSearchCV(pipe,param_grid,cv=20,n_jobs=-1,verbose=1,return_train_score=True,scoring='neg_mean_squared_error')#r2
  search=search.fit(X,y)# <== Fitter l'ensemble de donnÃ©es ici, sur l'ensemble != train test split
  print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))#Pas bon du tout !


  #svm_rmse_scores = np.sqrt(-scores)
  #explain_scores(svm_rmse_scores)

  print(search.best_params_)#{'extr__min_samples_leaf': 1, 'extr__n_estimators': 100, 'extr__random_state': 42, 'select__k': 2}


  if False:
    fn=list(X.columns.values)#more features than columns, why ?
    ft=pipe.named_steps['extr'].feature_importances_
    plot_feature_importances(ft,'w',fn,fn='re.png')

"""## model.predict(RandomTestSplitResults) 
- Meilleures performances sur Electricity-KBtu/Gfa que | Electricity(KBtu) pour certains modÃ¨les et vice-versa
- Cela prend bcp bcp de temps, dÃ©pendant des corrÃ©lations
"""

models={};r2ss={};bp={};prefix='';

#R2: 0.69 (+/- 0.47)
#globals().update(resume('prepa1'))
toPredict='Electricity-KBtu/Gfa'#Electricity(kBtu)
toPredict='Electricity(kBtu)'#E
print(toPredict)
prefix=filename(toPredict)

if(toPredict not in list(labelEncoded.columns.values)):
  labelEncoded[toPredict]=dfNewKeys[toPredict]

train=labelEncoded.drop([toPredict],axis=1);
for i in energies:
  if i in list(train.columns.values):
    train=train.drop([i],axis=1)
    print('drop : '+i)
X=train
test=y=labelEncoded[toPredict]
#df={}
df['gbfa'][1]
df['simple']=[X,y]

## model.predict(RandomTestSplitResults) .get r2 scores
columns=list(X.columns.values)
nbCols=len(columns)
#Faire un tableau du r2 score par modÃ¨le ( dÃ©placer l)
def modelisme(X, y, mdl, _pg, nbRealTest=10, GridSearchCVFolds=5, CrossValidationFolds=5, isObj=False, n_jobs=-1):
  global bp
#Perform Grid-Search
  a=time();
  if(isObj):
    cm=mdl
  else:
    cm=mdl()#constructeur

  gsc = GridSearchCV(estimator=cm,param_grid=_pg,cv=GridSearchCVFolds,scoring='r2',verbose=1, n_jobs=-1 )   #
  grid_search = gsc.fit(X, y)#Sur l'ensemble des donnÃ©es non splitÃ©es
  best_params = grid_search.best_params_ 
  print(grid_search.best_params_)
  print(time()-a);a=time()

#grid_search.feature_importances_
  if hasattr(grid_search,'feature_importances_'): 
    c1=len(grid_search.feature_importances_)
    c2=len(columns)
    if(c1 != c2):
      print('feature importance length:'+str(c1)+'<>'+str(c2))
    else:
      plot_feature_importances(grid_search.feature_importances_,'Volkov', list(X.columns.values),fn='RongoRongo.png')

#Puis Cross Validation : n passes sur ce jeu de donnÃ©es, faible variation : modÃ¨le solide, haute variation : pas bon du tout !    
  if(isObj):
    _mdl=mdl
  else:
    _mdl=mdl(**grid_search.best_params_)#max_depth=best_params["max_depth"], n_estimators=best_params["n_estimators"], random_state=False, verbose=False
  scores = sklearn.model_selection.cross_validate(_mdl, X, y, cv=CrossValidationFolds, scoring={'mse':'neg_mean_squared_error','r2':'r2'},verbose=1,n_jobs=-1)#r2'neg_mean_squared_error'
  #print("MSE: %0.2f (+/- %0.2f)" % (scores['test_mse'].mean(), scores['test_mse'].std() * 2))
  print("R2: %0.2f (+/- %0.2f)" % (scores['test_r2'].mean(), scores['test_r2'].std() * 2))
  print(time()-a);a=time()
  print('_'*180)
  #r2:0.76,0.43, neg mean:
  #predict=_mdl.predict(X)
  r2s=[]
  for i in list(range(0,nbRealTest)):
    x_train,y_train,x_test,y_test=sklearn.model_selection.train_test_split(X,y,train_size=0.8)
    _mdl.fit(x_train,x_test)
    predictions=_mdl.predict(y_train)
    r2=sklearn.metrics.r2_score(y_test,predictions)
    print(r2)#0.8663738878650655,0.33930533833303655
    r2s+=[r2]
  r2s=np.asarray(r2s);
  print("R2: %0.2f (+/- %0.2f)" % (r2s.mean(), r2s.std() * 2))#R2: 0.60 (+/- 0.53)
  print('_'*180)
  print(time()-a);a=time()
  return scores,_mdl,r2s

"""###RandomForestRegressor
- Probablement le plus robuste en l'occurence
"""

#toPredict='Electricity(kBtu)';train=X=labelEncoded.drop([toPredict],axis=1);test=y=labelEncoded[toPredict]
sub='RandomForestRegressor'
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X, y, sklearn.ensemble.RandomForestRegressor,{'max_depth':[1,30,60,100,300],'n_estimators':[10,300,600,1000]})#R2: 0.69 (+/- 0.47)

"""###decisiontreeregressor"""

sub='DecisionTreeRegressor'
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.tree.DecisionTreeRegressor,{'max_depth': range(2,40,3),'min_samples_split': range(2,40,3)})#R2: 0.65 (+/- 0.67)#criterion{â€œmseâ€, â€œfriedman_mseâ€, â€œmaeâ€}, default=â€mseâ€ #

"""###linearregression"""

sub='LinearRegression'
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.LinearRegression,{'normalize':[True,False]})#R2: 0.57 (+/- 0.33) plus robuste

"""###histgradientboost"""

sub='HistGradientBoostingRegressor';#input contains NaN with LR:20
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.ensemble.HistGradientBoostingRegressor,{'learning_rate':[0.0001,0.001,0.01,0.1,1,2]});#R2: 0.46 (+/- 0.38) best=R2-variance

"""###lasso"""

sub='Lasso';#input contains NaN with LR:20
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.ElasticNet,{'alpha': [0.1,1,10,20,30]});#R2: 0.46 (+/- 0.38)

"""###ridge"""

sub='Ridge';
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.ElasticNet,{'alpha':[0.1,1,10,20,30]});#R2: 0.59 (+/- 0.29)

"""###XGB"""

sub='XGBRegressor';
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,xgboost.XGBRegressor,{'objective':['reg:squarederror'],'learning_rate':[0.0001,0.001,0.01,0.1,1,2],#R2: 0.48 (+/- 0.51)
#'max_depth'        : [3, 15],#4, 5, 6, 8, 10, 12, 
#'min_child_weight' : [1, 7],#3, 5, 
#ValueError: Invalid parameter colsample_bytree for estimator GaussianNB(priors=None, var_smoothing=1e-09). Check the list of available parameters with `estimator.get_params().keys()`. 
#'gamma'            : [0.0,  0.4],#0.1, 0.2 , 0.3,
#'colsample_bytree' : [0.3,  0.7],#0.4, 0.5 ,
})

"""###gradbbost"""

sub='GradientBoostingRegressor';  #R2: 0.92 (+/- 0.09)
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.ensemble.GradientBoostingRegressor,{'learning_rate':[0.0001,0.001,0.01,0.1,1,2,10]});#R2: 0.72 (+/- 0.42)

"""###passagressive"""

sub='PassiveAggressiveRegressor';  
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.PassiveAggressiveRegressor,{'max_iter':[10,100,200,300,500,1000,2000,5000]});#R2: 0.44 (+/- 0.52)

"""###ortho"""

sub='OrthogonalMatchingPursuit';  
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.OrthogonalMatchingPursuit,{'n_nonzero_coefs':[None,round(nbCols*0.1),round(nbCols*0.2),round(nbCols*0.9)],'precompute':[True,False]});

"""###elasticnet"""

sub='ElasticNet';  
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.ElasticNet,{'alpha':[0.0001,0.01,1,10]});

"""###MLP"""

sub='MLPRegressor';#R2: 1.00 (+/- 0.00) whaow !
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.neural_network.MLPRegressor,{'activation':['relu'],'hidden_layer_sizes':[(100,)],'solver':['adam'],'learning_rate':['constant','adaptive'],'learning_rate_init':[0.1,0.001]});

"""###adaboost"""

sub='AdaBoostRegressor';#{'learning_rate': 0.01, 'loss': 'square', 'n_estimators': 600},  R2: 0.75 (+/- 0.36) pondÃ©rations Ã  la suite sur des stumps
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.ensemble.AdaBoostRegressor,{'n_estimators':[10,600],'learning_rate':[0.01,10],'loss':['linear','square']});

"""###kridge"""

sub='KernelRidge';#long Ã  calculer
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.kernel_ridge.KernelRidge,{'alpha':[0.0001,0.001,0.01,1,5,10]});#{'alpha': 1} en 1527 secondes, R2: 0.19 (+/- 2.07) = bof bof
message('KernelRidgeOver')

"""###kneigh"""

sub='KNeighborsRegressor';  
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.neighbors.KNeighborsRegressor,{'n_neighbors':[1,5,10,15,21,30,100]});

"""###huber"""

sub='HuberRegressor';  
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.HuberRegressor,{'alpha':[0.0001,0.01,1,10],'max_iter':[10,100,300,500],'epsilon':[1.35],'tol':[0.00001]});

"""###bayesianridge"""

sub='BayesianRidge';  
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.BayesianRidge,{'lambda_1':[0.000001,0.001,1,10]});

"""###lars"""

sub='Lars';  
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.Lars,{'n_nonzero_coefs':[1,500,np.inf]});

"""###lassolars"""

sub='LassoLars';#R2: 0.86 (+/- 0.07) Excellent !!!!!
scores,_mdl,r2ss[prefix+sub]=modelisme(X,y,sklearn.linear_model.LassoLars,{'alpha':[0.0001,0.01,1,10]});

"""###linearsvr"""

sub='LinearSVR';#'random_state':[0,42],
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.svm.LinearSVR,{'tol':[0.0001],'C':[1,100,400],'epsilon':[0,1,10],'loss':['epsilon_insensitive'],'fit_intercept':[True],'intercept_scaling':[1],'dual':[True],'verbose':[0],'random_state':[None],'max_iter':[1000]});

"""###svr"""

sub='SVR';#stable mais faible !
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.svm.SVR,{'kernel':['rbf'],'degree':[3],'gamma':['scale'],'coef0':[0.0],'tol':[0.001],'C':[1.0],'epsilon':[0.1],'shrinking':[True],'cache_size':[200],'verbose':[False],'max_iter':[-1]});

"""###SDGR"""

# Commented out IPython magic to ensure Python compatibility.
# %%script false 
# #Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data.
# #Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
# X_scaled=sklearn.preprocessing.StandardScaler()
# y_scaled=sklearn.preprocessing.StandardScaler()
# X_scaled.fit(X)
# y_scaled.fit(y)
# 
# sub='SGDRegressor';
# model=sklearn.linear_model.SGDRegressor
# scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X_scaled,y_scaled,model,
#                                             {'alpha':[0.0001,0.2,1,2,10],'eta0':[0.000001,0.01],'max_iter':[1000],'verbose':[1],'loss':['squared_loss'],'learning_rate':['optimal','invscaling'],'power_t':[0.25]});

"""###StackingRegressor"""

sub='StackingRegressor';#R2: 0.83 (+/- 0.33),
cvm=sklearn.ensemble.RandomForestRegressor(n_estimators=300);cv=5;cvm=None;#is ridgeCV
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,sklearn.ensemble.StackingRegressor([('Ridge',sklearn.linear_model.RidgeCV()),('BayesianRidge', sklearn.linear_model.BayesianRidge())], cvm,cv,n_jobs=-1),
                                       {'verbose':[1],'cv':[30,60]},
                                       nbRealTest=10,isObj=1);

"""###Stack2:Boosting
- Si l'on prenait
"""

sub='Stack2';
cvm=sklearn.ensemble.RandomForestRegressor(n_estimators=300);cv=5;cvm=None;#is ridgeCV
scores,models[prefix+sub],r2ss[prefix+sub]=modelisme(X,y,
sklearn.ensemble.StackingRegressor([
            ('XGB',xgboost.XGBRegressor(learning_rate=1,objective='reg:squarederror')),
            ('GradBoost', sklearn.ensemble.GradientBoostingRegressor(learning_rate=0.1)),
            ('HistGradBoost',sklearn.ensemble.HistGradientBoostingRegressor(learning_rate=0.1)),
            ], cvm,cv,n_jobs=-1),
{'verbose':[1],'cv':[30,60]},#cela prend bien du temps
nbRealTest=10,isObj=1);

"""###20 Real Test Shuffled Data on each Model"""

#models
message('final countdown began a long time ago :'+prefix)
r2sPerModel={}
nbRealTest=20
#print(prefix)
#models.keys()
mk=dict.fromkeys(models.keys(),[])

for i in mk:
  if 'simple----Ops' in i:
    nk='simple-'+re.sub(r"simple-*",'', i);
    #nk='simple-'+i.replace('simple-*','')#
    print(i+' => '+nk)
    models[nk]=models[i];
    del(models[i]);

  if (not i.startswith('electricity-kbtu-gfa')) & (not i.startswith('simple-')):
    nk='simple-'+i.replace('kbtu-','kbtu-')    
    models[nk]=models[i];del(models[i]);print(i+'=>'+nk)  

prefixs='simple-electricity-kbtu,electricity-kbtu-gfa'.split(',');
for prefix in prefixs:
  print('prefix is:',prefix)
  for j in list(range(0,nbRealTest)):
    print('_'*180)    
    if('gfa' in prefix):
      y=df['gbfa'][1];print('gfa')
    else: 
      y=df['simple'][1]

    x_train,y_train,x_test,y_test=sklearn.model_selection.train_test_split(X,y,train_size=0.8)
    for i in models:
      if not i.startswith(prefix):
        #print('skippin:not same prefix:'+i,i.startswith(prefix));
        continue;      
      if i == prefix+'SGDRegressor':
        continue;
      if(prefix+'-'+i not in r2sPerModel.keys()):
        r2sPerModel[prefix+'-'+i]=[]      
      models[i].fit(x_train,x_test)
      predictions=models[i].predict(y_train)
      if False:#rapporter Ã  Electricity-KBtu Total => tricher un peu, non ? car le r2 va Ãªtre plus fort ( l'Ã©cart vis Ã  vis de la mÃ©trique globale plus faible )
        if(toPredict=='Electricity-KBtu/Gfa'):
          predictions*=x_test['PropertyGFATotal']
          y_test*=x_test['PropertyGFATotal']

      r2=sklearn.metrics.r2_score(y_test,predictions)
      print(i+':'+str(round(r2,2)))#0.8663738878650655,0.33930533833303655
      r2sPerModel[prefix+'-'+i]+=[r2]

save(globals(),[],filename(toPredict),'r2sPerModel'.split(','))

"""- simple-electricity-kbtu-simple-electricity-kbtu-GradientBoostingRegressor R2: 0.78 (+/- 0.51)
- simple-electricity-kbtu-simple-electricity-kbtu-ElasticNet R2: 0.71 (+/- 0.18))
----
- electricity-kbtu-gfa-electricity-kbtu-gfaLinearRegression R2: 1.00 (+/- 0.00)
"""

#save(globals(),[],filename(toPredict),'r2sPerModel'.split(','))
for prefix in prefixs:
  print('_'*180)
  print(prefix)
  for i in r2sPerModel.keys():
    if(not i.startswith(prefix)):
      continue
    _x=np.asarray(r2sPerModel[i]);
    print(i,"R2: %0.2f (+/- %0.2f)" % (_x.mean(), _x.std() * 2))#R2: 0.60 (+/- 0.53)

#bagging = sklearn.ensemble.BaggingRegressor(sklearn.ensemble.ExtraTreesRegressor(),max_samples=0.5, max_features=0.5)
#voting = sklearn.ensemble.Voting(sklearn.ensemble.ExtraTreesRegressor())
assert(False)
globals().update(resume('checkpoint5'))#seattle
#-------------------------------------------------

from sklearn.pipeline import Pipeline
from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import RFECV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

toPredict='Electricity(kBtu)';train=X=labelEncoded.drop([toPredict],axis=1);test=y=labelEncoded[toPredict]
nbRuns=1
train=X=labelEncoded.drop([toPredict],axis=1);test=y=labelEncoded[toPredict]
#sklearn pipeline regressor hyper parameters
for i in range(1,nbRuns+1):
  X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(labelEncoded.drop([toPredict],axis=1),labelEncoded[toPredict],test_size=0.2)#,random_state=100

#this is the classifier used for feature selection
  rfr_featr_sele = sklearn.ensemble.RandomForestRegressor(n_estimators=30, random_state=42) #,class_weight="balanced"
  rfecv = RFECV(estimator=rfr_featr_sele, step=1, cv=5, scoring = 'r2')

#you can have different classifier for your final classifier
  rfr = sklearn.ensemble.RandomForestRegressor(n_estimators=10, random_state=42)#,class_weight="balanced"
  CV_rfc = GridSearchCV(rfr, param_grid={'max_depth':[2,3]},cv= 5, scoring = 'r2')#roc_auc: pour classification

  pipeline  = Pipeline([('feature_sele',rfecv),('rfr_cv',CV_rfc)])
  pipeline.fit(X_train, y_train)
  pipeline.predict(X_test)

  nmse=sklearn.metrics.mean_squared_error(y_test,predictions)
  r2=sklearn.metrics.r2_score(y_test,predictions)
  print(nmse)
  print(r2)
  #pipeline._best_params
  #pipeline._best_estimator

from sklearn.model_selection import ShuffleSplit
cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)
extr=clf = sklearn.ensemble.ExtraTreesRegressor()
scores = cross_val_score(clf, X, y, cv=cv, scoring='r2')
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))#Pas bon du tout !
#print(scores)#sur les n splits
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))#Pas bon du tout !

k_fold = sklearn.model_selection.KFold(n_splits=20)
scores = sklearn.model_selection.cross_val_score(clf,train,test, cv=k_fold, n_jobs=-1, scoring='r2')
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))#Accuracy: 0.94 (+/- 0.34)


from sklearn.feature_selection import SelectKBest#number of best features passed to the model


X=train=labelEncoded.drop(energies,axis=1)#newKeys
y=test=labelEncoded[toPredict]


'''
num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),('drop_attributes', AttributeDeleter()),('std_scaler', StandardScaler()),])
'''


important_names = fn[ft > np.mean(ft)]

len(ft)# len(X.columns.values);117, len(X.columns.values)#105
plot_feature_importances(ft,'w',fn,fn='re.png')
save(globals(),[],'1','ft,search,X,y'.split(','))


pipe.steps[1][1].fit(train,test)
extr.fit(train,test)
extr.feature_importances_

#pipe.steps[1][1]

print(search.best_params_)
print(search.cv_results_['mean_test_score'],search.cv_results_['mean_train_score'],search.cv_results_['params'])#

for train_indices, test_indices in k_fold.split(X):
  clf.fit(train_indices,test_indices)
  #print('Train: %s | test: %s' % (train_indices, test_indices))